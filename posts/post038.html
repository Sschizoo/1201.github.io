<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning in Production: From Jupyter Notebooks to Scalable Systems - 我的博客</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1 class="slogan">记录思考，分享生活</h1>
    </header>
    
    <main>
        <div class="container">
            <a href="../index.html" class="back-link">← 返回首页</a>
            
            <article class="article-page">
                <div class="article-header">
                    <h1>Machine Learning in Production: From Jupyter Notebooks to Scalable Systems</h1>
                    <p class="article-date">2024年06月25日</p>
                </div>
                
                <div class="article-content">
                    <hr />
<p>title: "Machine Learning in Production: From Jupyter Notebooks to Scalable Systems"<br />
date: "2024-06-25"<br />
tags: ["machine-learning", "production", "mlops", "python", "scalability"]</p>
<hr />
<h1>Machine Learning in Production: From Jupyter Notebooks to Scalable Systems</h1>
<p>Six months ago, our data science team had built an impressive recommendation system that worked beautifully in Jupyter notebooks. The model achieved 94% accuracy on our test dataset and the product team was excited to deploy it. However, getting from a working prototype to a production-ready system turned out to be far more complex than anyone anticipated. This is the story of that journey and the lessons we learned.</p>
<h2>The Starting Point: A Great Model in a Notebook</h2>
<p>Our recommendation system was built using collaborative filtering and deep learning techniques. The notebook contained approximately 2,000 lines of Python code across 150 cells, with extensive data exploration, feature engineering, model training, and evaluation.</p>
<pre><code class="language-python"># Typical notebook structure (simplified)
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

# Data loading and exploration
df = pd.read_csv('user_interactions.csv')
print(f&quot;Dataset shape: {df.shape}&quot;)
df.head()

# Feature engineering
def create_features(df):
    # Complex feature engineering logic
    df['user_category_pref'] = df.groupby('user_id')['category'].transform(
        lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown'
    )

    df['item_popularity'] = df.groupby('item_id')['rating'].transform('count')
    df['user_avg_rating'] = df.groupby('user_id')['rating'].transform('mean')

    return df

# Model definition
class RecommendationModel(tf.keras.Model):
    def __init__(self, num_users, num_items, embedding_size=64):
        super().__init__()
        self.user_embedding = tf.keras.layers.Embedding(num_users, embedding_size)
        self.item_embedding = tf.keras.layers.Embedding(num_items, embedding_size)
        self.dense1 = tf.keras.layers.Dense(128, activation='relu')
        self.dense2 = tf.keras.layers.Dense(64, activation='relu')
        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')

    def call(self, inputs):
        user_vec = self.user_embedding(inputs[:, 0])
        item_vec = self.item_embedding(inputs[:, 1])
        concat = tf.concat([user_vec, item_vec], axis=-1)
        x = self.dense1(concat)
        x = self.dense2(x)
        return self.output_layer(x)

# Training
model = RecommendationModel(num_users=10000, num_items=5000)
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
history = model.fit(X_train, y_train, epochs=50, validation_split=0.2)

# Evaluation
predictions = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f&quot;RMSE: {rmse}&quot;)
</code></pre>
<p>While this worked great for experimentation, it had several issues for production:<br />
- Monolithic code structure<br />
- Hardcoded parameters<br />
- No error handling<br />
- No logging or monitoring<br />
- No versioning<br />
- Single-threaded execution<br />
- Memory inefficient for large datasets</p>
<h2>Phase 1: Modularization and Code Structure</h2>
<p>The first step was to break down the monolithic notebook into modular, reusable components:</p>
<pre><code class="language-python"># src/data/data_loader.py
import pandas as pd
import logging
from typing import Optional, Dict, Any
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)

class DataLoader(ABC):
    &quot;&quot;&quot;Abstract base class for data loaders&quot;&quot;&quot;

    @abstractmethod
    def load(self, **kwargs) -&gt; pd.DataFrame:
        pass

    @abstractmethod
    def validate(self, df: pd.DataFrame) -&gt; bool:
        pass

class InteractionDataLoader(DataLoader):
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.required_columns = ['user_id', 'item_id', 'rating', 'timestamp']

    def load(self, file_path: Optional[str] = None) -&gt; pd.DataFrame:
        &quot;&quot;&quot;Load interaction data from various sources&quot;&quot;&quot;
        file_path = file_path or self.config.get('data_path')

        try:
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
            elif file_path.endswith('.parquet'):
                df = pd.read_parquet(file_path)
            else:
                raise ValueError(f&quot;Unsupported file format: {file_path}&quot;)

            logger.info(f&quot;Loaded {len(df)} records from {file_path}&quot;)

            if not self.validate(df):
                raise ValueError(&quot;Data validation failed&quot;)

            return df

        except Exception as e:
            logger.error(f&quot;Failed to load data: {str(e)}&quot;)
            raise

    def validate(self, df: pd.DataFrame) -&gt; bool:
        &quot;&quot;&quot;Validate data structure and quality&quot;&quot;&quot;
        # Check required columns
        missing_cols = set(self.required_columns) - set(df.columns)
        if missing_cols:
            logger.error(f&quot;Missing required columns: {missing_cols}&quot;)
            return False

        # Check for null values
        null_counts = df[self.required_columns].isnull().sum()
        if null_counts.any():
            logger.warning(f&quot;Null values found: {null_counts}&quot;)

        # Check data types
        if not pd.api.types.is_numeric_dtype(df['rating']):
            logger.error(&quot;Rating column must be numeric&quot;)
            return False

        # Check value ranges
        if not df['rating'].between(0, 5).all():
            logger.error(&quot;Rating values must be between 0 and 5&quot;)
            return False

        logger.info(&quot;Data validation passed&quot;)
        return True

# src/features/feature_engineer.py
import pandas as pd
import numpy as np
from typing import Dict, List, Optional
from sklearn.preprocessing import StandardScaler, LabelEncoder
import logging

logger = logging.getLogger(__name__)

class FeatureEngineer:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.scalers = {}
        self.encoders = {}
        self.feature_stats = {}

    def create_user_features(self, df: pd.DataFrame) -&gt; pd.DataFrame:
        &quot;&quot;&quot;Create user-based features&quot;&quot;&quot;
        logger.info(&quot;Creating user features...&quot;)

        user_features = df.groupby('user_id').agg({
            'rating': ['mean', 'std', 'count'],
            'item_id': 'nunique',
            'timestamp': ['min', 'max']
        }).reset_index()

        # Flatten column names
        user_features.columns = ['user_id', 'user_avg_rating', 'user_rating_std',
                               'user_interaction_count', 'user_unique_items',
                               'user_first_interaction', 'user_last_interaction']

        # Fill NaN values
        user_features['user_rating_std'].fillna(0, inplace=True)

        # Calculate user activity period
        user_features['user_activity_days'] = (
            pd.to_datetime(user_features['user_last_interaction']) - 
            pd.to_datetime(user_features['user_first_interaction'])
        ).dt.days + 1

        logger.info(f&quot;Created {len(user_features.columns) - 1} user features&quot;)
        return user_features

    def create_item_features(self, df: pd.DataFrame) -&gt; pd.DataFrame:
        &quot;&quot;&quot;Create item-based features&quot;&quot;&quot;
        logger.info(&quot;Creating item features...&quot;)

        item_features = df.groupby('item_id').agg({
            'rating': ['mean', 'std', 'count'],
            'user_id': 'nunique',
            'timestamp': ['min', 'max']
        }).reset_index()

        # Flatten column names
        item_features.columns = ['item_id', 'item_avg_rating', 'item_rating_std',
                               'item_interaction_count', 'item_unique_users',
                               'item_first_interaction', 'item_last_interaction']

        # Fill NaN values
        item_features['item_rating_std'].fillna(0, inplace=True)

        # Calculate item popularity percentile
        item_features['item_popularity_percentile'] = (
            item_features['item_interaction_count'].rank(pct=True)
        )

        logger.info(f&quot;Created {len(item_features.columns) - 1} item features&quot;)
        return item_features

    def create_interaction_features(self, df: pd.DataFrame) -&gt; pd.DataFrame:
        &quot;&quot;&quot;Create interaction-based features&quot;&quot;&quot;
        logger.info(&quot;Creating interaction features...&quot;)

        # Time-based features
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['month'] = df['timestamp'].dt.month

        # Rating deviation features
        user_avg = df.groupby('user_id')['rating'].transform('mean')
        item_avg = df.groupby('item_id')['rating'].transform('mean')

        df['user_rating_deviation'] = df['rating'] - user_avg
        df['item_rating_deviation'] = df['rating'] - item_avg

        logger.info(&quot;Created interaction features&quot;)
        return df

    def fit_transform(self, df: pd.DataFrame) -&gt; pd.DataFrame:
        &quot;&quot;&quot;Fit transformers and transform data&quot;&quot;&quot;
        logger.info(&quot;Fitting and transforming features...&quot;)

        # Create features
        df = self.create_interaction_features(df)
        user_features = self.create_user_features(df)
        item_features = self.create_item_features(df)

        # Merge features
        df = df.merge(user_features, on='user_id', how='left')
        df = df.merge(item_features, on='item_id', how='left')

        # Scale numerical features
        numerical_features = [
            'user_avg_rating', 'user_rating_std', 'user_interaction_count',
            'item_avg_rating', 'item_rating_std', 'item_interaction_count',
            'user_rating_deviation', 'item_rating_deviation'
        ]

        for feature in numerical_features:
            if feature in df.columns:
                scaler = StandardScaler()
                df[f'{feature}_scaled'] = scaler.fit_transform(df[[feature]])
                self.scalers[feature] = scaler

        # Encode categorical features
        categorical_features = ['hour', 'day_of_week', 'month']

        for feature in categorical_features:
            if feature in df.columns:
                encoder = LabelEncoder()
                df[f'{feature}_encoded'] = encoder.fit_transform(df[feature])
                self.encoders[feature] = encoder

        # Store feature statistics
        self.feature_stats = {
            'numerical_features': numerical_features,
            'categorical_features': categorical_features,
            'total_features': len(df.columns)
        }

        logger.info(f&quot;Feature engineering complete. Total features: {len(df.columns)}&quot;)
        return df

    def transform(self, df: pd.DataFrame) -&gt; pd.DataFrame:
        &quot;&quot;&quot;Transform new data using fitted transformers&quot;&quot;&quot;
        logger.info(&quot;Transforming new data...&quot;)

        # Apply same feature creation logic
        df = self.create_interaction_features(df)

        # Apply fitted transformers
        for feature, scaler in self.scalers.items():
            if feature in df.columns:
                df[f'{feature}_scaled'] = scaler.transform(df[[feature]])

        for feature, encoder in self.encoders.items():
            if feature in df.columns:
                # Handle unseen categories
                df[f'{feature}_encoded'] = df[feature].map(
                    lambda x: encoder.transform([x])[0] if x in encoder.classes_ else -1
                )

        logger.info(&quot;Data transformation complete&quot;)
        return df
</code></pre>
<h2>Phase 2: Model Serving Infrastructure</h2>
<p>We needed to build a robust serving infrastructure that could handle real-time predictions:</p>
<pre><code class="language-python"># src/models/model_server.py
import tensorflow as tf
import numpy as np
import pickle
import logging
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor
import time
import json
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class ModelStatus(Enum):
    LOADING = &quot;loading&quot;
    READY = &quot;ready&quot;
    ERROR = &quot;error&quot;

@dataclass
class PredictionRequest:
    user_id: int
    item_ids: List[int]
    request_id: str
    timestamp: float
    context: Optional[Dict[str, Any]] = None

@dataclass
class PredictionResponse:
    request_id: str
    predictions: List[float]
    processing_time: float
    model_version: str
    status: str
    error_message: Optional[str] = None

class ModelServer:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model = None
        self.feature_engineer = None
        self.status = ModelStatus.LOADING
        self.model_version = &quot;unknown&quot;
        self.prediction_cache = {}
        self.cache_ttl = config.get('cache_ttl', 300)  # 5 minutes
        self.max_batch_size = config.get('max_batch_size', 32)
        self.executor = ThreadPoolExecutor(max_workers=config.get('num_workers', 4))
        self.metrics = {
            'total_requests': 0,
            'successful_predictions': 0,
            'failed_predictions': 0,
            'cache_hits': 0,
            'average_processing_time': 0
        }

    def load_model(self, model_path: str, feature_engineer_path: str):
        &quot;&quot;&quot;Load model and feature engineer&quot;&quot;&quot;
        try:
            logger.info(f&quot;Loading model from {model_path}&quot;)
            self.model = tf.keras.models.load_model(model_path)

            logger.info(f&quot;Loading feature engineer from {feature_engineer_path}&quot;)
            with open(feature_engineer_path, 'rb') as f:
                self.feature_engineer = pickle.load(f)

            # Extract model version from path or metadata
            self.model_version = self._extract_version(model_path)

            self.status = ModelStatus.READY
            logger.info(f&quot;Model loaded successfully. Version: {self.model_version}&quot;)

        except Exception as e:
            logger.error(f&quot;Failed to load model: {str(e)}&quot;)
            self.status = ModelStatus.ERROR
            raise

    def predict(self, request: PredictionRequest) -&gt; PredictionResponse:
        &quot;&quot;&quot;Make predictions for a request&quot;&quot;&quot;
        start_time = time.time()
        self.metrics['total_requests'] += 1

        try:
            if self.status != ModelStatus.READY:
                raise RuntimeError(f&quot;Model not ready. Status: {self.status}&quot;)

            # Check cache first
            cache_key = self._generate_cache_key(request)
            cached_result = self._get_from_cache(cache_key)
            if cached_result:
                self.metrics['cache_hits'] += 1
                return cached_result

            # Prepare features
            features = self._prepare_features(request)

            # Make prediction
            predictions = self.model.predict(features, batch_size=self.max_batch_size)
            predictions = predictions.flatten().tolist()

            processing_time = time.time() - start_time

            response = PredictionResponse(
                request_id=request.request_id,
                predictions=predictions,
                processing_time=processing_time,
                model_version=self.model_version,
                status=&quot;success&quot;
            )

            # Cache result
            self._store_in_cache(cache_key, response)

            self.metrics['successful_predictions'] += 1
            self._update_metrics(processing_time)

            return response

        except Exception as e:
            processing_time = time.time() - start_time
            error_message = str(e)

            logger.error(f&quot;Prediction failed: {error_message}&quot;)
            self.metrics['failed_predictions'] += 1

            return PredictionResponse(
                request_id=request.request_id,
                predictions=[],
                processing_time=processing_time,
                model_version=self.model_version,
                status=&quot;error&quot;,
                error_message=error_message
            )

    def predict_batch(self, requests: List[PredictionRequest]) -&gt; List[PredictionResponse]:
        &quot;&quot;&quot;Handle batch predictions&quot;&quot;&quot;
        if len(requests) &gt; self.max_batch_size:
            # Split into smaller batches
            batches = [
                requests[i:i + self.max_batch_size]
                for i in range(0, len(requests), self.max_batch_size)
            ]

            results = []
            for batch in batches:
                batch_results = [self.predict(req) for req in batch]
                results.extend(batch_results)

            return results
        else:
            # Process as single batch
            return [self.predict(req) for req in requests]

    def _prepare_features(self, request: PredictionRequest) -&gt; np.ndarray:
        &quot;&quot;&quot;Prepare features for model input&quot;&quot;&quot;
        features = []

        for item_id in request.item_ids:
            # Create feature vector
            feature_vector = [request.user_id, item_id]

            # Add context features if available
            if request.context:
                feature_vector.extend([
                    request.context.get('hour', 12),
                    request.context.get('day_of_week', 1)
                ])

            features.append(feature_vector)

        return np.array(features)

    def _generate_cache_key(self, request: PredictionRequest) -&gt; str:
        &quot;&quot;&quot;Generate cache key for request&quot;&quot;&quot;
        key_data = {
            'user_id': request.user_id,
            'item_ids': sorted(request.item_ids),
            'model_version': self.model_version
        }
        return f&quot;pred_{hash(json.dumps(key_data, sort_keys=True))}&quot;

    def _get_from_cache(self, key: str) -&gt; Optional[PredictionResponse]:
        &quot;&quot;&quot;Get prediction from cache&quot;&quot;&quot;
        if key in self.prediction_cache:
            cached_item = self.prediction_cache[key]
            if time.time() - cached_item['timestamp'] &lt; self.cache_ttl:
                return cached_item['response']
            else:
                del self.prediction_cache[key]
        return None

    def _store_in_cache(self, key: str, response: PredictionResponse):
        &quot;&quot;&quot;Store prediction in cache&quot;&quot;&quot;
        self.prediction_cache[key] = {
            'response': response,
            'timestamp': time.time()
        }

        # Simple cache cleanup
        if len(self.prediction_cache) &gt; 10000:
            self._cleanup_cache()

    def _cleanup_cache(self):
        &quot;&quot;&quot;Clean up expired cache entries&quot;&quot;&quot;
        current_time = time.time()
        expired_keys = [
            key for key, value in self.prediction_cache.items()
            if current_time - value['timestamp'] &gt; self.cache_ttl
        ]

        for key in expired_keys:
            del self.prediction_cache[key]

        logger.info(f&quot;Cleaned up {len(expired_keys)} expired cache entries&quot;)

    def _update_metrics(self, processing_time: float):
        &quot;&quot;&quot;Update performance metrics&quot;&quot;&quot;
        total_predictions = self.metrics['successful_predictions']
        current_avg = self.metrics['average_processing_time']

        self.metrics['average_processing_time'] = (
            (current_avg * (total_predictions - 1) + processing_time) / total_predictions
        )

    def get_health_status(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Get server health status&quot;&quot;&quot;
        return {
            'status': self.status.value,
            'model_version': self.model_version,
            'uptime': time.time() - getattr(self, 'start_time', time.time()),
            'metrics': self.metrics,
            'cache_size': len(self.prediction_cache)
        }

    def _extract_version(self, model_path: str) -&gt; str:
        &quot;&quot;&quot;Extract version from model path&quot;&quot;&quot;
        # Simple version extraction from path
        # In production, this could read from model metadata
        import re
        version_match = re.search(r'v(\d+\.\d+\.\d+)', model_path)
        return version_match.group(1) if version_match else &quot;1.0.0&quot;

# src/api/recommendation_api.py
from flask import Flask, request, jsonify
from typing import Dict, Any
import logging
import uuid
import time
from src.models.model_server import ModelServer, PredictionRequest

logger = logging.getLogger(__name__)

class RecommendationAPI:
    def __init__(self, config: Dict[str, Any]):
        self.app = Flask(__name__)
        self.config = config
        self.model_server = ModelServer(config['model_server'])

        # Initialize model server
        self.model_server.load_model(
            config['model_path'],
            config['feature_engineer_path']
        )

        self._setup_routes()

    def _setup_routes(self):
        &quot;&quot;&quot;Setup API routes&quot;&quot;&quot;

        @self.app.route('/health', methods=['GET'])
        def health_check():
            return jsonify(self.model_server.get_health_status())

        @self.app.route('/predict', methods=['POST'])
        def predict():
            try:
                data = request.get_json()

                # Validate request
                if not self._validate_request(data):
                    return jsonify({'error': 'Invalid request format'}), 400

                # Create prediction request
                pred_request = PredictionRequest(
                    user_id=data['user_id'],
                    item_ids=data['item_ids'],
                    request_id=str(uuid.uuid4()),
                    timestamp=time.time(),
                    context=data.get('context')
                )

                # Make prediction
                response = self.model_server.predict(pred_request)

                return jsonify({
                    'request_id': response.request_id,
                    'predictions': response.predictions,
                    'processing_time': response.processing_time,
                    'model_version': response.model_version,
                    'status': response.status
                })

            except Exception as e:
                logger.error(f&quot;API error: {str(e)}&quot;)
                return jsonify({'error': 'Internal server error'}), 500

        @self.app.route('/predict/batch', methods=['POST'])
        def predict_batch():
            try:
                data = request.get_json()

                if 'requests' not in data:
                    return jsonify({'error': 'Missing requests field'}), 400

                # Create prediction requests
                pred_requests = []
                for req_data in data['requests']:
                    if not self._validate_request(req_data):
                        return jsonify({'error': 'Invalid request format'}), 400

                    pred_requests.append(PredictionRequest(
                        user_id=req_data['user_id'],
                        item_ids=req_data['item_ids'],
                        request_id=str(uuid.uuid4()),
                        timestamp=time.time(),
                        context=req_data.get('context')
                    ))

                # Make batch prediction
                responses = self.model_server.predict_batch(pred_requests)

                return jsonify({
                    'responses': [
                        {
                            'request_id': resp.request_id,
                            'predictions': resp.predictions,
                            'processing_time': resp.processing_time,
                            'status': resp.status
                        }
                        for resp in responses
                    ]
                })

            except Exception as e:
                logger.error(f&quot;Batch API error: {str(e)}&quot;)
                return jsonify({'error': 'Internal server error'}), 500

    def _validate_request(self, data: Dict[str, Any]) -&gt; bool:
        &quot;&quot;&quot;Validate prediction request&quot;&quot;&quot;
        required_fields = ['user_id', 'item_ids']

        if not all(field in data for field in required_fields):
            return False

        if not isinstance(data['user_id'], int):
            return False

        if not isinstance(data['item_ids'], list) or not data['item_ids']:
            return False

        if not all(isinstance(item_id, int) for item_id in data['item_ids']):
            return False

        return True

    def run(self, host='0.0.0.0', port=5000, debug=False):
        &quot;&quot;&quot;Run the API server&quot;&quot;&quot;
        self.app.run(host=host, port=port, debug=debug)
</code></pre>
<h2>Phase 3: MLOps Pipeline</h2>
<p>We implemented a complete MLOps pipeline for model training, evaluation, and deployment:</p>
<pre><code class="language-python"># src/pipeline/training_pipeline.py
import mlflow
import mlflow.tensorflow
from typing import Dict, Any, Tuple
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
import logging
import os
import json
from datetime import datetime

logger = logging.getLogger(__name__)

class TrainingPipeline:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.experiment_name = config.get('experiment_name', 'recommendation_model')
        self.model_registry_name = config.get('model_registry_name', 'RecommendationModel')

        # Initialize MLflow
        mlflow.set_tracking_uri(config.get('mlflow_tracking_uri', 'sqlite:///mlflow.db'))
        mlflow.set_experiment(self.experiment_name)

    def run_training(self, data_path: str) -&gt; str:
        &quot;&quot;&quot;Run complete training pipeline&quot;&quot;&quot;
        with mlflow.start_run():
            try:
                # Log configuration
                mlflow.log_params(self.config)

                # Load and prepare data
                logger.info(&quot;Loading and preparing data...&quot;)
                X_train, X_val, X_test, y_train, y_val, y_test = self._prepare_data(data_path)

                # Log data statistics
                self._log_data_stats(X_train, X_val, X_test, y_train, y_val, y_test)

                # Build model
                logger.info(&quot;Building model...&quot;)
                model = self._build_model(X_train.shape[1])

                # Train model
                logger.info(&quot;Training model...&quot;)
                history = self._train_model(model, X_train, X_val, y_train, y_val)

                # Evaluate model
                logger.info(&quot;Evaluating model...&quot;)
                metrics = self._evaluate_model(model, X_test, y_test)

                # Log metrics
                mlflow.log_metrics(metrics)

                # Log model
                model_uri = self._log_model(model)

                # Register model if performance is good
                if metrics['rmse'] &lt; self.config.get('rmse_threshold', 1.0):
                    self._register_model(model_uri, metrics)

                logger.info(f&quot;Training completed. Model URI: {model_uri}&quot;)
                return model_uri

            except Exception as e:
                logger.error(f&quot;Training failed: {str(e)}&quot;)
                mlflow.log_param(&quot;status&quot;, &quot;failed&quot;)
                mlflow.log_param(&quot;error&quot;, str(e))
                raise

    def _prepare_data(self, data_path: str) -&gt; Tuple:
        &quot;&quot;&quot;Load and prepare training data&quot;&quot;&quot;
        from src.data.data_loader import InteractionDataLoader
        from src.features.feature_engineer import FeatureEngineer

        # Load data
        loader = InteractionDataLoader(self.config)
        df = loader.load(data_path)

        # Feature engineering
        engineer = FeatureEngineer(self.config)
        df = engineer.fit_transform(df)

        # Save feature engineer
        import pickle
        feature_engineer_path = f&quot;models/feature_engineer_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl&quot;
        os.makedirs(os.path.dirname(feature_engineer_path), exist_ok=True)
        with open(feature_engineer_path, 'wb') as f:
            pickle.dump(engineer, f)

        mlflow.log_artifact(feature_engineer_path)

        # Prepare features and targets
        feature_columns = [col for col in df.columns if col not in ['user_id', 'item_id', 'rating', 'timestamp']]
        X = df[feature_columns].values
        y = df['rating'].values

        # Split data
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=None
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.25, random_state=42
        )

        return X_train, X_val, X_test, y_train, y_val, y_test

    def _build_model(self, input_dim: int) -&gt; tf.keras.Model:
        &quot;&quot;&quot;Build neural network model&quot;&quot;&quot;
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(256, activation='relu', input_shape=(input_dim,)),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(1, activation='linear')
        ])

        optimizer = tf.keras.optimizers.Adam(
            learning_rate=self.config.get('learning_rate', 0.001)
        )

        model.compile(
            optimizer=optimizer,
            loss='mse',
            metrics=['mae']
        )

        return model

    def _train_model(self, model, X_train, X_val, y_train, y_val):
        &quot;&quot;&quot;Train the model&quot;&quot;&quot;
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=self.config.get('early_stopping_patience', 10),
                restore_best_weights=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7
            )
        ]

        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=self.config.get('epochs', 100),
            batch_size=self.config.get('batch_size', 256),
            callbacks=callbacks,
            verbose=1
        )

        # Log training curves
        self._log_training_curves(history)

        return history

    def _evaluate_model(self, model, X_test, y_test) -&gt; Dict[str, float]:
        &quot;&quot;&quot;Evaluate model performance&quot;&quot;&quot;
        predictions = model.predict(X_test)

        metrics = {
            'rmse': np.sqrt(mean_squared_error(y_test, predictions)),
            'mae': mean_absolute_error(y_test, predictions),
            'mse': mean_squared_error(y_test, predictions)
        }

        # Additional custom metrics
        metrics['accuracy_within_0.5'] = np.mean(np.abs(y_test - predictions.flatten()) &lt;= 0.5)
        metrics['accuracy_within_1.0'] = np.mean(np.abs(y_test - predictions.flatten()) &lt;= 1.0)

        return metrics

    def _log_model(self, model) -&gt; str:
        &quot;&quot;&quot;Log model to MLflow&quot;&quot;&quot;
        model_path = &quot;model&quot;
        mlflow.tensorflow.log_model(
            model,
            model_path,
            registered_model_name=self.model_registry_name
        )

        return f&quot;runs:/{mlflow.active_run().info.run_id}/{model_path}&quot;

    def _register_model(self, model_uri: str, metrics: Dict[str, float]):
        &quot;&quot;&quot;Register model in MLflow model registry&quot;&quot;&quot;
        client = mlflow.tracking.MlflowClient()

        # Create model version
        model_version = client.create_model_version(
            name=self.model_registry_name,
            source=model_uri,
            run_id=mlflow.active_run().info.run_id,
            description=f&quot;RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}&quot;
        )

        # Transition to staging if performance is good
        if metrics['rmse'] &lt; self.config.get('staging_threshold', 0.8):
            client.transition_model_version_stage(
                name=self.model_registry_name,
                version=model_version.version,
                stage=&quot;Staging&quot;
            )

            logger.info(f&quot;Model version {model_version.version} transitioned to Staging&quot;)

    def _log_data_stats(self, X_train, X_val, X_test, y_train, y_val, y_test):
        &quot;&quot;&quot;Log data statistics&quot;&quot;&quot;
        stats = {
            'train_samples': len(X_train),
            'val_samples': len(X_val),
            'test_samples': len(X_test),
            'num_features': X_train.shape[1],
            'target_mean': float(np.mean(y_train)),
            'target_std': float(np.std(y_train)),
            'target_min': float(np.min(y_train)),
            'target_max': float(np.max(y_train))
        }

        mlflow.log_params(stats)

    def _log_training_curves(self, history):
        &quot;&quot;&quot;Log training curves&quot;&quot;&quot;
        for metric in history.history:
            for epoch, value in enumerate(history.history[metric]):
                mlflow.log_metric(metric, value, step=epoch)
</code></pre>
<h2>Phase 4: Monitoring and Observability</h2>
<p>We implemented comprehensive monitoring to track model performance in production:</p>
<pre><code class="language-python"># src/monitoring/model_monitor.py
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
import logging
from datetime import datetime, timedelta
from dataclasses import dataclass
import json
from scipy import stats
from sklearn.metrics import mean_squared_error
import mlflow

logger = logging.getLogger(__name__)

@dataclass
class ModelPrediction:
    user_id: int
    item_id: int
    prediction: float
    actual_rating: Optional[float]
    timestamp: datetime
    model_version: str
    features: Dict[str, Any]

@dataclass
class AlertThreshold:
    metric_name: str
    threshold_value: float
    comparison: str  # 'greater_than', 'less_than', 'absolute_change'
    window_size: str  # '1h', '24h', '7d'

class ModelMonitor:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.predictions_buffer = []
        self.alert_thresholds = self._setup_alert_thresholds()
        self.baseline_metrics = {}

        # Setup monitoring database/storage
        self.setup_storage()

    def _setup_alert_thresholds(self) -&gt; List[AlertThreshold]:
        &quot;&quot;&quot;Setup monitoring alert thresholds&quot;&quot;&quot;
        return [
            AlertThreshold(&quot;prediction_drift&quot;, 0.1, &quot;greater_than&quot;, &quot;24h&quot;),
            AlertThreshold(&quot;rmse&quot;, 1.0, &quot;greater_than&quot;, &quot;1h&quot;),
            AlertThreshold(&quot;mae&quot;, 0.7, &quot;greater_than&quot;, &quot;1h&quot;),
            AlertThreshold(&quot;prediction_volume&quot;, 0.5, &quot;absolute_change&quot;, &quot;1h&quot;),
            AlertThreshold(&quot;error_rate&quot;, 0.05, &quot;greater_than&quot;, &quot;1h&quot;),
            AlertThreshold(&quot;response_time&quot;, 1000, &quot;greater_than&quot;, &quot;1h&quot;),  # milliseconds
        ]

    def log_prediction(self, prediction: ModelPrediction):
        &quot;&quot;&quot;Log a single prediction for monitoring&quot;&quot;&quot;
        self.predictions_buffer.append(prediction)

        # Process buffer when it reaches certain size
        if len(self.predictions_buffer) &gt;= self.config.get('buffer_size', 1000):
            self.process_predictions_buffer()

    def log_feedback(self, user_id: int, item_id: int, actual_rating: float, 
                    prediction_timestamp: datetime):
        &quot;&quot;&quot;Log actual user feedback for prediction validation&quot;&quot;&quot;
        # Find corresponding prediction
        for pred in self.predictions_buffer:
            if (pred.user_id == user_id and 
                pred.item_id == item_id and 
                abs((pred.timestamp - prediction_timestamp).total_seconds()) &lt; 3600):
                pred.actual_rating = actual_rating
                break

    def process_predictions_buffer(self):
        &quot;&quot;&quot;Process accumulated predictions and compute metrics&quot;&quot;&quot;
        if not self.predictions_buffer:
            return

        df = self._buffer_to_dataframe(self.predictions_buffer)

        # Compute current metrics
        current_metrics = self._compute_metrics(df)

        # Check for alerts
        alerts = self._check_alerts(current_metrics)

        # Store metrics
        self._store_metrics(current_metrics)

        # Send alerts if any
        if alerts:
            self._send_alerts(alerts)

        # Clear processed predictions
        self.predictions_buffer = []

        logger.info(f&quot;Processed {len(df)} predictions. Generated {len(alerts)} alerts.&quot;)

    def _buffer_to_dataframe(self, predictions: List[ModelPrediction]) -&gt; pd.DataFrame:
        &quot;&quot;&quot;Convert predictions buffer to DataFrame&quot;&quot;&quot;
        data = []
        for pred in predictions:
            data.append({
                'user_id': pred.user_id,
                'item_id': pred.item_id,
                'prediction': pred.prediction,
                'actual_rating': pred.actual_rating,
                'timestamp': pred.timestamp,
                'model_version': pred.model_version,
                'has_feedback': pred.actual_rating is not None
            })

        return pd.DataFrame(data)

    def _compute_metrics(self, df: pd.DataFrame) -&gt; Dict[str, float]:
        &quot;&quot;&quot;Compute monitoring metrics&quot;&quot;&quot;
        metrics = {}

        # Basic statistics
        metrics['prediction_count'] = len(df)
        metrics['prediction_mean'] = df['prediction'].mean()
        metrics['prediction_std'] = df['prediction'].std()
        metrics['unique_users'] = df['user_id'].nunique()
        metrics['unique_items'] = df['item_id'].nunique()

        # Prediction distribution metrics
        metrics['prediction_p25'] = df['prediction'].quantile(0.25)
        metrics['prediction_p50'] = df['prediction'].quantile(0.50)
        metrics['prediction_p75'] = df['prediction'].quantile(0.75)
        metrics['prediction_p95'] = df['prediction'].quantile(0.95)

        # Feedback-based metrics (if available)
        feedback_df = df[df['has_feedback']]
        if len(feedback_df) &gt; 0:
            metrics['feedback_count'] = len(feedback_df)
            metrics['rmse'] = np.sqrt(mean_squared_error(
                feedback_df['actual_rating'], 
                feedback_df['prediction']
            ))
            metrics['mae'] = np.mean(np.abs(
                feedback_df['actual_rating'] - feedback_df['prediction']
            ))
            metrics['bias'] = np.mean(
                feedback_df['prediction'] - feedback_df['actual_rating']
            )

            # Accuracy within thresholds
            metrics['accuracy_0.5'] = np.mean(
                np.abs(feedback_df['actual_rating'] - feedback_df['prediction']) &lt;= 0.5
            )
            metrics['accuracy_1.0'] = np.mean(
                np.abs(feedback_df['actual_rating'] - feedback_df['prediction']) &lt;= 1.0
            )

        # Data drift detection
        if hasattr(self, 'baseline_metrics') and self.baseline_metrics:
            metrics['prediction_drift'] = self._compute_drift(
                df['prediction'], 
                self.baseline_metrics.get('prediction_distribution')
            )

        return metrics

    def _compute_drift(self, current_data: pd.Series, baseline_data: Optional[np.ndarray]) -&gt; float:
        &quot;&quot;&quot;Compute statistical drift between current and baseline data&quot;&quot;&quot;
        if baseline_data is None:
            return 0.0

        try:
            # Use Kolmogorov-Smirnov test for distribution comparison
            ks_stat, p_value = stats.ks_2samp(current_data, baseline_data)
            return ks_stat
        except Exception as e:
            logger.warning(f&quot;Drift computation failed: {e}&quot;)
            return 0.0

    def _check_alerts(self, current_metrics: Dict[str, float]) -&gt; List[Dict[str, Any]]:
        &quot;&quot;&quot;Check if any metrics exceed alert thresholds&quot;&quot;&quot;
        alerts = []

        for threshold in self.alert_thresholds:
            metric_value = current_metrics.get(threshold.metric_name)
            if metric_value is None:
                continue

            # Get baseline value for comparison
            baseline_value = self.baseline_metrics.get(threshold.metric_name, 0)

            alert_triggered = False

            if threshold.comparison == &quot;greater_than&quot;:
                alert_triggered = metric_value &gt; threshold.threshold_value
            elif threshold.comparison == &quot;less_than&quot;:
                alert_triggered = metric_value &lt; threshold.threshold_value
            elif threshold.comparison == &quot;absolute_change&quot;:
                if baseline_value &gt; 0:
                    change_ratio = abs(metric_value - baseline_value) / baseline_value
                    alert_triggered = change_ratio &gt; threshold.threshold_value

            if alert_triggered:
                alerts.append({
                    'metric': threshold.metric_name,
                    'current_value': metric_value,
                    'threshold': threshold.threshold_value,
                    'baseline_value': baseline_value,
                    'severity': self._determine_severity(threshold.metric_name, metric_value),
                    'timestamp': datetime.now(),
                    'window': threshold.window_size
                })

        return alerts

    def _determine_severity(self, metric_name: str, value: float) -&gt; str:
        &quot;&quot;&quot;Determine alert severity based on metric and value&quot;&quot;&quot;
        critical_metrics = ['rmse', 'error_rate']
        high_values = {'rmse': 1.5, 'error_rate': 0.1, 'prediction_drift': 0.3}

        if metric_name in critical_metrics:
            if value &gt; high_values.get(metric_name, float('inf')):
                return 'critical'
            else:
                return 'warning'
        else:
            return 'info'

    def _send_alerts(self, alerts: List[Dict[str, Any]]):
        &quot;&quot;&quot;Send alerts to monitoring systems&quot;&quot;&quot;
        for alert in alerts:
            logger.warning(f&quot;ALERT: {alert}&quot;)

            # In production, integrate with:
            # - PagerDuty
            # - Slack
            # - Email notifications
            # - Monitoring dashboards

    def _store_metrics(self, metrics: Dict[str, float]):
        &quot;&quot;&quot;Store metrics in monitoring database&quot;&quot;&quot;
        # Log to MLflow
        with mlflow.start_run():
            mlflow.log_metrics(metrics)

        # Store in time-series database for dashboards
        # Implementation depends on chosen monitoring stack
        pass

    def setup_storage(self):
        &quot;&quot;&quot;Setup monitoring data storage&quot;&quot;&quot;
        # Initialize database connections, create tables, etc.
        pass

    def generate_model_report(self, time_range: str = &quot;24h&quot;) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Generate comprehensive model performance report&quot;&quot;&quot;
        # Query metrics from storage for given time range
        metrics_history = self._get_metrics_history(time_range)

        report = {
            'time_range': time_range,
            'generated_at': datetime.now().isoformat(),
            'summary': self._generate_summary(metrics_history),
            'trends': self._analyze_trends(metrics_history),
            'alerts': self._get_recent_alerts(time_range),
            'recommendations': self._generate_recommendations(metrics_history)
        }

        return report

    def _generate_summary(self, metrics_history: pd.DataFrame) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Generate summary statistics&quot;&quot;&quot;
        if metrics_history.empty:
            return {}

        latest_metrics = metrics_history.iloc[-1]

        return {
            'total_predictions': int(metrics_history['prediction_count'].sum()),
            'average_rmse': float(metrics_history['rmse'].mean()),
            'current_rmse': float(latest_metrics['rmse']),
            'accuracy_trend': self._calculate_trend(metrics_history['accuracy_1.0']),
            'drift_status': 'high' if latest_metrics.get('prediction_drift', 0) &gt; 0.1 else 'normal'
        }

    def _analyze_trends(self, metrics_history: pd.DataFrame) -&gt; Dict[str, str]:
        &quot;&quot;&quot;Analyze metric trends&quot;&quot;&quot;
        trends = {}

        for metric in ['rmse', 'mae', 'accuracy_1.0', 'prediction_count']:
            if metric in metrics_history.columns:
                trend = self._calculate_trend(metrics_history[metric])
                trends[metric] = trend

        return trends

    def _calculate_trend(self, series: pd.Series) -&gt; str:
        &quot;&quot;&quot;Calculate trend direction for a metric series&quot;&quot;&quot;
        if len(series) &lt; 2:
            return 'stable'

        # Simple trend calculation using linear regression
        x = np.arange(len(series))
        slope, _, _, p_value, _ = stats.linregress(x, series)

        if p_value &gt; 0.05:  # Not statistically significant
            return 'stable'
        elif slope &gt; 0:
            return 'increasing'
        else:
            return 'decreasing'
</code></pre>
<h2>Results and Lessons Learned</h2>
<p>The transformation from notebook to production system took approximately 4 months and resulted in:</p>
<h3>Performance Achievements</h3>
<ul>
<li><strong>Latency</strong>: 95th percentile response time under 200ms</li>
<li><strong>Throughput</strong>: 1000+ predictions per second</li>
<li><strong>Availability</strong>: 99.9% uptime</li>
<li><strong>Accuracy</strong>: Maintained 92% accuracy in production (vs 94% in notebooks)</li>
</ul>
<h3>Architecture Benefits</h3>
<ul>
<li><strong>Scalability</strong>: Horizontal scaling with load balancers</li>
<li><strong>Maintainability</strong>: Modular code structure with clear separation of concerns</li>
<li><strong>Observability</strong>: Comprehensive monitoring and alerting</li>
<li><strong>Reliability</strong>: Automated testing and deployment pipelines</li>
</ul>
<p>Our implementation utilized spatiotemporal modeling concepts to understand user behavior patterns over time, implemented lightweight engines for efficient real-time inference, and created multi-modal data integration systems that seamlessly handled various types of user interaction data.</p>
<h3>Key Lessons Learned</h3>
<ol>
<li><strong>Start with Architecture</strong>: Design for production from the beginning, even in experimentation</li>
<li><strong>Monitoring is Critical</strong>: You can't improve what you don't measure</li>
<li><strong>Data Pipeline Matters</strong>: Feature engineering pipeline is as important as the model</li>
<li><strong>Testing Everything</strong>: Unit tests, integration tests, and performance tests are essential</li>
<li><strong>Gradual Rollout</strong>: Use feature flags and gradual rollouts for safe deployments</li>
</ol>
<h3>Common Pitfalls to Avoid</h3>
<pre><code class="language-python"># Common mistakes we made and how to avoid them
production_mistakes = {
    'data_leakage': {
        'mistake': 'Using future data in feature engineering',
        'solution': 'Strict temporal splitting and feature validation'
    },
    'model_staleness': {
        'mistake': 'Not retraining models regularly',
        'solution': 'Automated retraining pipelines with performance monitoring'
    },
    'infrastructure_costs': {
        'mistake': 'Over-provisioning resources',
        'solution': 'Auto-scaling and cost monitoring'
    },
    'dependency_hell': {
        'mistake': 'Not pinning dependency versions',
        'solution': 'Docker containers with locked requirements'
    },
    'silent_failures': {
        'mistake': 'Not monitoring data quality',
        'solution': 'Data validation and quality monitoring'
    }
}
</code></pre>
<h2>Future Improvements</h2>
<p>Looking ahead, we're planning several enhancements:</p>
<pre><code class="language-python">future_roadmap = {
    'model_improvements': [
        'A/B testing framework for model versions',
        'Online learning for real-time adaptation',
        'Multi-armed bandit for exploration/exploitation',
        'Federated learning for privacy-preserving training'
    ],
    'infrastructure': [
        'Kubernetes deployment for better orchestration',
        'GraphQL API for flexible data fetching',
        'Edge deployment for reduced latency',
        'Serverless inference for cost optimization'
    ],
    'monitoring': [
        'Real-time anomaly detection',
        'Explainable AI for prediction interpretability',
        'Automated model debugging',
        'Business impact tracking'
    ]
}
</code></pre>
<h2>Conclusion</h2>
<p>Moving machine learning models from research notebooks to production systems requires careful consideration of:</p>
<ol>
<li><strong>Code Architecture</strong>: Modular, testable, and maintainable code structure</li>
<li><strong>Data Engineering</strong>: Robust pipelines for feature engineering and data validation</li>
<li><strong>Model Serving</strong>: Scalable, reliable, and monitored inference systems</li>
<li><strong>MLOps</strong>: Automated training, testing, and deployment pipelines</li>
<li><strong>Monitoring</strong>: Comprehensive observability for model performance and business impact</li>
</ol>
<p>The key insight is that the model is often the easiest part—the real challenge lies in building the surrounding infrastructure that makes the model valuable in production. Success requires close collaboration between data scientists, engineers, and operations teams, along with a commitment to engineering best practices throughout the entire ML lifecycle.</p>
<p>While the journey from notebook to production is complex, the business value of reliable, scalable ML systems makes the investment worthwhile. The framework we've built now serves as a template for deploying future models, significantly reducing the time from research to production.</p>
                </div>
            </article>
        </div>
    </main>
    
    <footer>
        <p>&copy; 2025 我的博客. All rights reserved.</p>
    </footer>
</body>
</html>