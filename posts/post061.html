<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building Scalable APIs: From Monolith to Microservices - 我的博客</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1 class="slogan">记录思考，分享生活</h1>
    </header>
    
    <main>
        <div class="container">
            <a href="../index.html" class="back-link">← 返回首页</a>
            
            <article class="article-page">
                <div class="article-header">
                    <h1>Building Scalable APIs: From Monolith to Microservices</h1>
                    <p class="article-date">2024年12月12日</p>
                </div>
                
                <div class="article-content">
                    <hr />
<p>title: "Building Scalable APIs: From Monolith to Microservices"<br />
date: "2024-12-12"<br />
tags: ["API Design", "Microservices", "Architecture", "Scalability", "System Design"]</p>
<hr />
<h1>Building Scalable APIs: From Monolith to Microservices</h1>
<h2>Introduction</h2>
<p>Two years ago, our team was maintaining a monolithic API that served over 100,000 daily active users. While it had served us well during our startup phase, we were hitting the limits of what a single application could handle. Database connections were maxed out, deployment cycles were risky, and different parts of our system had conflicting scaling needs. This is the story of how we successfully transitioned from a monolithic architecture to a microservices ecosystem, and the lessons we learned along the way.</p>
<h2>Chapter 1: The Monolithic Foundation</h2>
<h3>The Original Architecture</h3>
<p>Our monolithic API was built with Node.js and Express, following a traditional MVC pattern:</p>
<pre><code class="language-javascript">// app.js - The heart of our monolith
const express = require('express');
const cors = require('cors');
const helmet = require('helmet');
const rateLimit = require('express-rate-limit');

const app = express();

// Middleware
app.use(helmet());
app.use(cors());
app.use(express.json({ limit: '10mb' }));
app.use(express.urlencoded({ extended: true }));

// Rate limiting
const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100 // limit each IP to 100 requests per windowMs
});
app.use('/api/', limiter);

// Routes
app.use('/api/users', require('./routes/users'));
app.use('/api/products', require('./routes/products'));
app.use('/api/orders', require('./routes/orders'));
app.use('/api/payments', require('./routes/payments'));
app.use('/api/analytics', require('./routes/analytics'));
app.use('/api/notifications', require('./routes/notifications'));

// Error handling
app.use((err, req, res, next) =&gt; {
  console.error(err.stack);
  res.status(500).json({ error: 'Something went wrong!' });
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () =&gt; {
  console.log(`Server running on port ${PORT}`);
});
</code></pre>
<h3>The Growing Pains</h3>
<p>As our application grew, several issues became apparent:</p>
<p><strong>Database Bottlenecks:</strong></p>
<pre><code class="language-javascript">// Database connection pool getting exhausted
const mysql = require('mysql2/promise');

const pool = mysql.createPool({
  host: process.env.DB_HOST,
  user: process.env.DB_USER,
  password: process.env.DB_PASSWORD,
  database: process.env.DB_NAME,
  waitForConnections: true,
  connectionLimit: 10, // This became insufficient
  queueLimit: 0
});

// Heavy queries blocking the pool
const getAnalytics = async (req, res) =&gt; {
  try {
    // This query takes 30+ seconds during peak hours
    const [rows] = await pool.execute(`
      SELECT 
        DATE(created_at) as date,
        COUNT(*) as orders,
        SUM(total_amount) as revenue,
        AVG(total_amount) as avg_order_value
      FROM orders o
      JOIN users u ON o.user_id = u.id
      WHERE o.created_at &gt;= DATE_SUB(NOW(), INTERVAL 1 YEAR)
      GROUP BY DATE(created_at)
      ORDER BY date DESC
    `);

    res.json(rows);
  } catch (error) {
    res.status(500).json({ error: 'Analytics query failed' });
  }
};
</code></pre>
<p><strong>Deployment Risks:</strong></p>
<pre><code class="language-yaml"># deploy.yml - All-or-nothing deployment
version: '3.8'
services:
  app:
    image: our-monolith:latest
    ports:
      - &quot;3000:3000&quot;
    environment:
      - NODE_ENV=production
      - DB_HOST=db
      - REDIS_HOST=redis
    depends_on:
      - db
      - redis

  # Single point of failure
  db:
    image: mysql:8.0
    environment:
      - MYSQL_ROOT_PASSWORD=secret
      - MYSQL_DATABASE=ourapp

  redis:
    image: redis:alpine
</code></pre>
<p><strong>Scaling Challenges:</strong><br />
- Payment processing needed high reliability but low volume<br />
- Analytics queries were resource-intensive<br />
- User management required different scaling patterns than order processing<br />
- Image processing was CPU-intensive and blocked other requests</p>
<h2>Chapter 2: Planning the Migration</h2>
<h3>Service Decomposition Strategy</h3>
<p>We analyzed our monolith and identified natural service boundaries:</p>
<pre><code class="language-mermaid">graph TB
    A[User Management] --&gt; B[Authentication]
    C[Product Catalog] --&gt; D[Search]
    E[Order Processing] --&gt; F[Payment]
    E --&gt; G[Inventory]
    H[Analytics] --&gt; I[Reporting]
    J[Notifications] --&gt; K[Email]
    J --&gt; L[SMS]
</code></pre>
<h3>The Strangler Fig Pattern</h3>
<p>We decided to use the Strangler Fig pattern to gradually replace parts of the monolith:</p>
<pre><code class="language-javascript">// API Gateway routing configuration
const routes = [
  {
    path: '/api/users/*',
    target: process.env.USER_SERVICE_URL || 'http://localhost:3000',
    service: 'users'
  },
  {
    path: '/api/products/*',
    target: process.env.PRODUCT_SERVICE_URL || 'http://localhost:3000',
    service: 'products'
  },
  {
    path: '/api/orders/*',
    target: process.env.ORDER_SERVICE_URL || 'http://localhost:3000',
    service: 'orders'
  },
  // Fallback to monolith for non-migrated endpoints
  {
    path: '/api/*',
    target: process.env.MONOLITH_URL || 'http://localhost:3000',
    service: 'monolith'
  }
];
</code></pre>
<h3>Database Decomposition</h3>
<p>One of the most challenging aspects was breaking apart the shared database:</p>
<pre><code class="language-sql">-- Original monolithic database schema
CREATE TABLE users (
  id INT PRIMARY KEY AUTO_INCREMENT,
  username VARCHAR(50) UNIQUE NOT NULL,
  email VARCHAR(100) UNIQUE NOT NULL,
  password_hash VARCHAR(255) NOT NULL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE products (
  id INT PRIMARY KEY AUTO_INCREMENT,
  name VARCHAR(200) NOT NULL,
  description TEXT,
  price DECIMAL(10,2) NOT NULL,
  inventory_count INT DEFAULT 0,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE orders (
  id INT PRIMARY KEY AUTO_INCREMENT,
  user_id INT NOT NULL,
  total_amount DECIMAL(10,2) NOT NULL,
  status ENUM('pending', 'processing', 'shipped', 'delivered', 'cancelled'),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (user_id) REFERENCES users(id)
);

CREATE TABLE order_items (
  id INT PRIMARY KEY AUTO_INCREMENT,
  order_id INT NOT NULL,
  product_id INT NOT NULL,
  quantity INT NOT NULL,
  price DECIMAL(10,2) NOT NULL,
  FOREIGN KEY (order_id) REFERENCES orders(id),
  FOREIGN KEY (product_id) REFERENCES products(id)
);
</code></pre>
<p>We needed to carefully plan data migration and handle cross-service transactions.</p>
<h2>Chapter 3: The First Microservice</h2>
<h3>User Service Implementation</h3>
<p>We started with the User Service as it had the fewest dependencies:</p>
<pre><code class="language-javascript">// user-service/src/app.js
const express = require('express');
const jwt = require('jsonwebtoken');
const bcrypt = require('bcrypt');
const { Pool } = require('pg');

const app = express();

// Database connection for user service
const pool = new Pool({
  host: process.env.POSTGRES_HOST,
  port: process.env.POSTGRES_PORT,
  database: process.env.POSTGRES_DB,
  user: process.env.POSTGRES_USER,
  password: process.env.POSTGRES_PASSWORD,
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

// Middleware
app.use(express.json());
app.use(require('./middleware/auth'));

// User registration
app.post('/api/users/register', async (req, res) =&gt; {
  const { username, email, password } = req.body;

  try {
    // Input validation
    if (!username || !email || !password) {
      return res.status(400).json({ error: 'Missing required fields' });
    }

    // Check if user exists
    const existingUser = await pool.query(
      'SELECT id FROM users WHERE username = $1 OR email = $2',
      [username, email]
    );

    if (existingUser.rows.length &gt; 0) {
      return res.status(409).json({ error: 'User already exists' });
    }

    // Hash password
    const saltRounds = 12;
    const passwordHash = await bcrypt.hash(password, saltRounds);

    // Create user
    const result = await pool.query(
      'INSERT INTO users (username, email, password_hash) VALUES ($1, $2, $3) RETURNING id, username, email, created_at',
      [username, email, passwordHash]
    );

    const user = result.rows[0];

    // Generate JWT token
    const token = jwt.sign(
      { userId: user.id, username: user.username },
      process.env.JWT_SECRET,
      { expiresIn: '24h' }
    );

    res.status(201).json({
      message: 'User created successfully',
      user: {
        id: user.id,
        username: user.username,
        email: user.email,
        createdAt: user.created_at
      },
      token
    });

  } catch (error) {
    console.error('Registration error:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

// User authentication
app.post('/api/users/login', async (req, res) =&gt; {
  const { username, password } = req.body;

  try {
    // Find user
    const result = await pool.query(
      'SELECT id, username, email, password_hash FROM users WHERE username = $1',
      [username]
    );

    if (result.rows.length === 0) {
      return res.status(401).json({ error: 'Invalid credentials' });
    }

    const user = result.rows[0];

    // Verify password
    const isPasswordValid = await bcrypt.compare(password, user.password_hash);

    if (!isPasswordValid) {
      return res.status(401).json({ error: 'Invalid credentials' });
    }

    // Generate JWT token
    const token = jwt.sign(
      { userId: user.id, username: user.username },
      process.env.JWT_SECRET,
      { expiresIn: '24h' }
    );

    res.json({
      message: 'Login successful',
      user: {
        id: user.id,
        username: user.username,
        email: user.email
      },
      token
    });

  } catch (error) {
    console.error('Login error:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

// Health check
app.get('/health', (req, res) =&gt; {
  res.json({ status: 'healthy', service: 'user-service' });
});

const PORT = process.env.PORT || 3001;
app.listen(PORT, () =&gt; {
  console.log(`User service running on port ${PORT}`);
});
</code></pre>
<h3>Service Communication</h3>
<p>We implemented HTTP-based communication with circuit breakers:</p>
<pre><code class="language-javascript">// shared/http-client.js
const axios = require('axios');
const CircuitBreaker = require('opossum');

class ServiceClient {
  constructor(baseURL, options = {}) {
    this.client = axios.create({
      baseURL,
      timeout: options.timeout || 5000,
      headers: {
        'Content-Type': 'application/json'
      }
    });

    // Circuit breaker configuration
    const circuitOptions = {
      timeout: options.timeout || 5000,
      errorThresholdPercentage: 50,
      resetTimeout: 30000,
      rollingCountTimeout: 10000,
      rollingCountBuckets: 10
    };

    this.breaker = new CircuitBreaker(this.makeRequest.bind(this), circuitOptions);

    // Circuit breaker events
    this.breaker.on('open', () =&gt; {
      console.log(`Circuit breaker opened for ${baseURL}`);
    });

    this.breaker.on('halfOpen', () =&gt; {
      console.log(`Circuit breaker half-open for ${baseURL}`);
    });

    this.breaker.on('close', () =&gt; {
      console.log(`Circuit breaker closed for ${baseURL}`);
    });
  }

  async makeRequest(config) {
    try {
      const response = await this.client(config);
      return response.data;
    } catch (error) {
      console.error(`Service request failed: ${error.message}`);
      throw error;
    }
  }

  async get(url, config = {}) {
    return this.breaker.fire({ ...config, method: 'GET', url });
  }

  async post(url, data, config = {}) {
    return this.breaker.fire({ ...config, method: 'POST', url, data });
  }

  async put(url, data, config = {}) {
    return this.breaker.fire({ ...config, method: 'PUT', url, data });
  }

  async delete(url, config = {}) {
    return this.breaker.fire({ ...config, method: 'DELETE', url });
  }
}

module.exports = ServiceClient;
</code></pre>
<h3>Service Discovery</h3>
<p>We implemented a simple service registry:</p>
<pre><code class="language-javascript">// service-registry/index.js
const express = require('express');
const redis = require('redis');

const app = express();
const client = redis.createClient(process.env.REDIS_URL);

app.use(express.json());

// Service registration
app.post('/register', async (req, res) =&gt; {
  const { serviceName, serviceUrl, healthCheckUrl } = req.body;

  try {
    const serviceKey = `service:${serviceName}`;
    const serviceData = {
      url: serviceUrl,
      healthCheck: healthCheckUrl,
      registeredAt: new Date().toISOString(),
      lastSeen: new Date().toISOString()
    };

    await client.setex(serviceKey, 60, JSON.stringify(serviceData));

    res.json({ message: 'Service registered successfully' });
  } catch (error) {
    res.status(500).json({ error: 'Registration failed' });
  }
});

// Service discovery
app.get('/discover/:serviceName', async (req, res) =&gt; {
  const { serviceName } = req.params;

  try {
    const serviceKey = `service:${serviceName}`;
    const serviceData = await client.get(serviceKey);

    if (!serviceData) {
      return res.status(404).json({ error: 'Service not found' });
    }

    res.json(JSON.parse(serviceData));
  } catch (error) {
    res.status(500).json({ error: 'Discovery failed' });
  }
});

// Health check endpoint
app.get('/health', (req, res) =&gt; {
  res.json({ status: 'healthy', service: 'service-registry' });
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () =&gt; {
  console.log(`Service registry running on port ${PORT}`);
});
</code></pre>
<h2>Chapter 4: API Gateway Implementation</h2>
<h3>Gateway Architecture</h3>
<p>We built a comprehensive API Gateway to handle cross-cutting concerns:</p>
<pre><code class="language-javascript">// api-gateway/src/app.js
const express = require('express');
const httpProxy = require('http-proxy-middleware');
const rateLimit = require('express-rate-limit');
const helmet = require('helmet');
const cors = require('cors');
const jwt = require('jsonwebtoken');

const app = express();

// Security middleware
app.use(helmet());
app.use(cors());
app.use(express.json());

// Rate limiting
const createRateLimiter = (windowMs, max) =&gt; {
  return rateLimit({
    windowMs,
    max,
    message: 'Too many requests from this IP, please try again later.'
  });
};

// Authentication middleware
const authenticateToken = (req, res, next) =&gt; {
  const authHeader = req.headers['authorization'];
  const token = authHeader &amp;&amp; authHeader.split(' ')[1];

  if (!token) {
    return res.status(401).json({ error: 'Access token required' });
  }

  jwt.verify(token, process.env.JWT_SECRET, (err, user) =&gt; {
    if (err) {
      return res.status(403).json({ error: 'Invalid token' });
    }
    req.user = user;
    next();
  });
};

// Service routes configuration
const serviceRoutes = [
  {
    path: '/api/users',
    target: process.env.USER_SERVICE_URL,
    rateLimit: createRateLimiter(15 * 60 * 1000, 100),
    auth: false // Registration and login don't require auth
  },
  {
    path: '/api/products',
    target: process.env.PRODUCT_SERVICE_URL,
    rateLimit: createRateLimiter(15 * 60 * 1000, 200),
    auth: false // Product catalog is public
  },
  {
    path: '/api/orders',
    target: process.env.ORDER_SERVICE_URL,
    rateLimit: createRateLimiter(15 * 60 * 1000, 50),
    auth: true // Orders require authentication
  },
  {
    path: '/api/payments',
    target: process.env.PAYMENT_SERVICE_URL,
    rateLimit: createRateLimiter(15 * 60 * 1000, 10),
    auth: true // Payments require authentication
  },
  {
    path: '/api/analytics',
    target: process.env.ANALYTICS_SERVICE_URL,
    rateLimit: createRateLimiter(15 * 60 * 1000, 20),
    auth: true // Analytics require authentication
  }
];

// Setup proxy routes
serviceRoutes.forEach(route =&gt; {
  const middlewares = [];

  // Add rate limiting
  if (route.rateLimit) {
    middlewares.push(route.rateLimit);
  }

  // Add authentication
  if (route.auth) {
    middlewares.push(authenticateToken);
  }

  // Add proxy
  middlewares.push(
    httpProxy({
      target: route.target,
      changeOrigin: true,
      onError: (err, req, res) =&gt; {
        console.error(`Proxy error for ${route.path}:`, err.message);
        res.status(503).json({ error: 'Service unavailable' });
      },
      onProxyReq: (proxyReq, req, res) =&gt; {
        // Add user context to downstream services
        if (req.user) {
          proxyReq.setHeader('X-User-ID', req.user.userId);
          proxyReq.setHeader('X-User-Name', req.user.username);
        }
      }
    })
  );

  app.use(route.path, ...middlewares);
});

// Health check aggregation
app.get('/health', async (req, res) =&gt; {
  const services = serviceRoutes.map(route =&gt; ({
    name: route.path.replace('/api/', ''),
    url: route.target
  }));

  const healthChecks = await Promise.allSettled(
    services.map(async service =&gt; {
      try {
        const response = await fetch(`${service.url}/health`);
        const data = await response.json();
        return {
          service: service.name,
          status: data.status,
          healthy: response.ok
        };
      } catch (error) {
        return {
          service: service.name,
          status: 'unhealthy',
          healthy: false,
          error: error.message
        };
      }
    })
  );

  const results = healthChecks.map(result =&gt; result.value);
  const allHealthy = results.every(result =&gt; result.healthy);

  res.status(allHealthy ? 200 : 503).json({
    status: allHealthy ? 'healthy' : 'unhealthy',
    services: results,
    timestamp: new Date().toISOString()
  });
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () =&gt; {
  console.log(`API Gateway running on port ${PORT}`);
});
</code></pre>
<h2>Chapter 5: Advanced Patterns</h2>
<h3>Event-Driven Architecture</h3>
<p>We implemented event-driven communication for loosely coupled services:</p>
<pre><code class="language-javascript">// shared/event-bus.js
const EventEmitter = require('events');
const redis = require('redis');

class EventBus extends EventEmitter {
  constructor() {
    super();
    this.publisher = redis.createClient(process.env.REDIS_URL);
    this.subscriber = redis.createClient(process.env.REDIS_URL);

    this.subscriber.on('message', (channel, message) =&gt; {
      try {
        const event = JSON.parse(message);
        this.emit(event.type, event);
      } catch (error) {
        console.error('Failed to parse event:', error);
      }
    });
  }

  async publish(eventType, data) {
    const event = {
      type: eventType,
      data,
      timestamp: new Date().toISOString(),
      id: `${Date.now()}-${Math.random().toString(36).substr(2, 9)}`
    };

    await this.publisher.publish('events', JSON.stringify(event));
    console.log(`Event published: ${eventType}`, event.id);
  }

  async subscribe(eventType, handler) {
    this.on(eventType, handler);
    await this.subscriber.subscribe('events');
  }
}

module.exports = new EventBus();
</code></pre>
<h3>Saga Pattern for Distributed Transactions</h3>
<pre><code class="language-javascript">// order-service/src/sagas/orderSaga.js
const eventBus = require('../../shared/event-bus');

class OrderSaga {
  constructor() {
    this.setupEventHandlers();
  }

  setupEventHandlers() {
    eventBus.subscribe('order.created', this.handleOrderCreated.bind(this));
    eventBus.subscribe('payment.processed', this.handlePaymentProcessed.bind(this));
    eventBus.subscribe('payment.failed', this.handlePaymentFailed.bind(this));
    eventBus.subscribe('inventory.reserved', this.handleInventoryReserved.bind(this));
    eventBus.subscribe('inventory.insufficient', this.handleInventoryInsufficient.bind(this));
  }

  async handleOrderCreated(event) {
    const { orderId, userId, items, totalAmount } = event.data;

    try {
      // Step 1: Reserve inventory
      await eventBus.publish('inventory.reserve', {
        orderId,
        items
      });

      // Step 2: Process payment
      await eventBus.publish('payment.process', {
        orderId,
        userId,
        amount: totalAmount
      });

    } catch (error) {
      await this.compensateOrder(orderId, 'order_creation_failed');
    }
  }

  async handlePaymentProcessed(event) {
    const { orderId, paymentId } = event.data;

    try {
      // Update order status
      await orderService.updateOrderStatus(orderId, 'paid');

      // Trigger fulfillment
      await eventBus.publish('order.fulfill', {
        orderId,
        paymentId
      });

    } catch (error) {
      await this.compensateOrder(orderId, 'payment_processing_failed');
    }
  }

  async handlePaymentFailed(event) {
    const { orderId, reason } = event.data;

    // Compensate: Release inventory and cancel order
    await eventBus.publish('inventory.release', { orderId });
    await orderService.updateOrderStatus(orderId, 'cancelled');
  }

  async handleInventoryInsufficient(event) {
    const { orderId, items } = event.data;

    // Compensate: Cancel order
    await orderService.updateOrderStatus(orderId, 'cancelled');
    await eventBus.publish('order.cancelled', {
      orderId,
      reason: 'insufficient_inventory'
    });
  }

  async compensateOrder(orderId, reason) {
    await eventBus.publish('inventory.release', { orderId });
    await eventBus.publish('payment.refund', { orderId });
    await orderService.updateOrderStatus(orderId, 'cancelled');
  }
}

module.exports = new OrderSaga();
</code></pre>
<h2>Chapter 6: Data Management Challenges</h2>
<h3>Database Per Service</h3>
<p>Each service got its own database:</p>
<pre><code class="language-yaml"># docker-compose.yml
version: '3.8'
services:
  user-service:
    build: ./user-service
    environment:
      - POSTGRES_HOST=user-db
      - POSTGRES_DB=users
    depends_on:
      - user-db

  user-db:
    image: postgres:13
    environment:
      - POSTGRES_DB=users
      - POSTGRES_USER=userservice
      - POSTGRES_PASSWORD=secret
    volumes:
      - user-data:/var/lib/postgresql/data

  product-service:
    build: ./product-service
    environment:
      - POSTGRES_HOST=product-db
      - POSTGRES_DB=products
    depends_on:
      - product-db

  product-db:
    image: postgres:13
    environment:
      - POSTGRES_DB=products
      - POSTGRES_USER=productservice
      - POSTGRES_PASSWORD=secret
    volumes:
      - product-data:/var/lib/postgresql/data

  order-service:
    build: ./order-service
    environment:
      - POSTGRES_HOST=order-db
      - POSTGRES_DB=orders
    depends_on:
      - order-db

  order-db:
    image: postgres:13
    environment:
      - POSTGRES_DB=orders
      - POSTGRES_USER=orderservice
      - POSTGRES_PASSWORD=secret
    volumes:
      - order-data:/var/lib/postgresql/data

volumes:
  user-data:
  product-data:
  order-data:
</code></pre>
<h3>Data Consistency Patterns</h3>
<p>We implemented eventual consistency with event sourcing:</p>
<pre><code class="language-javascript">// shared/event-store.js
class EventStore {
  constructor(database) {
    this.db = database;
  }

  async appendEvent(streamId, eventType, eventData, expectedVersion) {
    const event = {
      stream_id: streamId,
      event_type: eventType,
      event_data: JSON.stringify(eventData),
      event_version: expectedVersion + 1,
      created_at: new Date()
    };

    try {
      const result = await this.db.query(
        'INSERT INTO events (stream_id, event_type, event_data, event_version, created_at) VALUES ($1, $2, $3, $4, $5) RETURNING *',
        [event.stream_id, event.event_type, event.event_data, event.event_version, event.created_at]
      );

      return result.rows[0];
    } catch (error) {
      if (error.code === '23505') { // Unique constraint violation
        throw new Error('Concurrency conflict');
      }
      throw error;
    }
  }

  async getEvents(streamId, fromVersion = 0) {
    const result = await this.db.query(
      'SELECT * FROM events WHERE stream_id = $1 AND event_version &gt; $2 ORDER BY event_version ASC',
      [streamId, fromVersion]
    );

    return result.rows.map(row =&gt; ({
      ...row,
      event_data: JSON.parse(row.event_data)
    }));
  }

  async getEventsByType(eventType, fromTimestamp = null) {
    const query = fromTimestamp
      ? 'SELECT * FROM events WHERE event_type = $1 AND created_at &gt; $2 ORDER BY created_at ASC'
      : 'SELECT * FROM events WHERE event_type = $1 ORDER BY created_at ASC';

    const params = fromTimestamp ? [eventType, fromTimestamp] : [eventType];
    const result = await this.db.query(query, params);

    return result.rows.map(row =&gt; ({
      ...row,
      event_data: JSON.parse(row.event_data)
    }));
  }
}

module.exports = EventStore;
</code></pre>
<h2>Chapter 7: Observability and Monitoring</h2>
<h3>Distributed Tracing</h3>
<p>We implemented distributed tracing to track requests across services:</p>
<pre><code class="language-javascript">// shared/tracing.js
const { NodeTracerProvider } = require('@opentelemetry/sdk-node');
const { Resource } = require('@opentelemetry/resources');
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');
const { JaegerExporter } = require('@opentelemetry/exporter-jaeger');
const { BatchSpanProcessor } = require('@opentelemetry/sdk-trace-base');
const { registerInstrumentations } = require('@opentelemetry/instrumentation');
const { HttpInstrumentation } = require('@opentelemetry/instrumentation-http');
const { ExpressInstrumentation } = require('@opentelemetry/instrumentation-express');

function initTracing(serviceName) {
  const provider = new NodeTracerProvider({
    resource: new Resource({
      [SemanticResourceAttributes.SERVICE_NAME]: serviceName,
      [SemanticResourceAttributes.SERVICE_VERSION]: process.env.SERVICE_VERSION || '1.0.0',
    }),
  });

  const exporter = new JaegerExporter({
    endpoint: process.env.JAEGER_ENDPOINT || 'http://localhost:14268/api/traces',
  });

  provider.addSpanProcessor(new BatchSpanProcessor(exporter));
  provider.register();

  registerInstrumentations({
    instrumentations: [
      new HttpInstrumentation(),
      new ExpressInstrumentation(),
    ],
  });

  return provider;
}

module.exports = { initTracing };
</code></pre>
<h3>Centralized Logging</h3>
<pre><code class="language-javascript">// shared/logger.js
const winston = require('winston');
const { ElasticsearchTransport } = require('winston-elasticsearch');

const createLogger = (serviceName) =&gt; {
  const transports = [
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.simple()
      )
    })
  ];

  if (process.env.ELASTICSEARCH_URL) {
    transports.push(
      new ElasticsearchTransport({
        level: 'info',
        clientOpts: {
          node: process.env.ELASTICSEARCH_URL
        },
        index: `logs-${serviceName}`,
        transformer: (logData) =&gt; {
          return {
            '@timestamp': new Date().toISOString(),
            service: serviceName,
            level: logData.level,
            message: logData.message,
            ...logData.meta
          };
        }
      })
    );
  }

  const logger = winston.createLogger({
    level: process.env.LOG_LEVEL || 'info',
    format: winston.format.combine(
      winston.format.timestamp(),
      winston.format.errors({ stack: true }),
      winston.format.json()
    ),
    defaultMeta: { service: serviceName },
    transports
  });

  return logger;
};

module.exports = { createLogger };
</code></pre>
<h2>Chapter 8: Complex Data Processing Requirements</h2>
<h3>Handling Spatiotemporal Data</h3>
<p>In our migration journey, we encountered unique challenges when dealing with location-based analytics and real-time data processing. We needed to implement lightweight engines capable of efficient spatiotemporal modeling while maintaining the ability to integrate multi-modal data sources.</p>
<p>The solution involved creating specialized microservices that could handle:<br />
- Real-time location tracking with temporal correlation<br />
- Historical data analysis with spatial dimensions<br />
- Multi-modal data integration from various sensors and user inputs<br />
- Complex queries requiring both spatial and temporal indexing</p>
<p>This experience taught us that microservices architecture becomes even more valuable when dealing with specialized data processing requirements, as it allows for targeted optimization of each service's data handling capabilities.</p>
<h2>Chapter 9: Performance Results</h2>
<h3>Before vs After Migration</h3>
<p>The results of our migration were impressive:</p>
<p><strong>Scalability Improvements:</strong><br />
- <strong>Request Handling</strong>: From 1,000 to 10,000 concurrent requests<br />
- <strong>Database Performance</strong>: 75% reduction in query response time<br />
- <strong>Memory Usage</strong>: 60% reduction per service (better resource allocation)<br />
- <strong>CPU Utilization</strong>: More efficient distribution across services</p>
<p><strong>Reliability Improvements:</strong><br />
- <strong>Uptime</strong>: From 99.5% to 99.9%<br />
- <strong>MTTR</strong>: From 2 hours to 15 minutes<br />
- <strong>Deployment Frequency</strong>: From weekly to daily releases<br />
- <strong>Rollback Time</strong>: From 1 hour to 5 minutes</p>
<p><strong>Development Velocity:</strong><br />
- <strong>Feature Delivery</strong>: 40% faster development cycles<br />
- <strong>Team Productivity</strong>: Teams can work independently<br />
- <strong>Bug Resolution</strong>: Faster isolation and fixes<br />
- <strong>Code Quality</strong>: Better separation of concerns</p>
<h3>Performance Metrics</h3>
<pre><code class="language-javascript">// performance-monitoring/metrics.js
const promClient = require('prom-client');

// Create metrics registry
const register = new promClient.Registry();

// Add default metrics
promClient.collectDefaultMetrics({ register });

// Custom metrics
const httpRequestDuration = new promClient.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code', 'service'],
  buckets: [0.1, 0.5, 1, 2, 5, 10]
});

const httpRequestsTotal = new promClient.Counter({
  name: 'http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status_code', 'service']
});

const activeConnections = new promClient.Gauge({
  name: 'active_connections',
  help: 'Number of active connections',
  labelNames: ['service']
});

register.registerMetric(httpRequestDuration);
register.registerMetric(httpRequestsTotal);
register.registerMetric(activeConnections);

module.exports = {
  register,
  httpRequestDuration,
  httpRequestsTotal,
  activeConnections
};
</code></pre>
<h2>Chapter 10: Lessons Learned</h2>
<h3>What Worked Well</h3>
<ol>
<li><strong>Gradual Migration</strong>: The Strangler Fig pattern allowed us to migrate incrementally</li>
<li><strong>Database Separation</strong>: Each service having its own database improved performance</li>
<li><strong>Event-Driven Architecture</strong>: Loose coupling improved system resilience</li>
<li><strong>API Gateway</strong>: Centralized cross-cutting concerns simplified service development</li>
<li><strong>Comprehensive Monitoring</strong>: Observability was crucial for debugging distributed systems</li>
</ol>
<h3>What We'd Do Differently</h3>
<ol>
<li><strong>Start with Monitoring</strong>: We should have implemented comprehensive monitoring before migration</li>
<li><strong>Data Migration Strategy</strong>: We underestimated the complexity of data migration</li>
<li><strong>Testing Strategy</strong>: Integration testing became more complex and should have been planned earlier</li>
<li><strong>Documentation</strong>: Service contracts and API documentation became critical</li>
</ol>
<h3>Key Takeaways</h3>
<ol>
<li><strong>Conway's Law</strong>: Your system architecture will mirror your organization structure</li>
<li><strong>Complexity Trade-offs</strong>: You're trading monolithic complexity for distributed system complexity</li>
<li><strong>Team Size Matters</strong>: Microservices make more sense with larger teams</li>
<li><strong>Operational Overhead</strong>: Be prepared for increased operational complexity</li>
<li><strong>Cultural Change</strong>: Success requires changes in team culture and processes</li>
</ol>
<h2>Conclusion</h2>
<p>The journey from monolith to microservices was challenging but ultimately rewarding. We achieved our goals of improved scalability, reliability, and development velocity, but it came with increased operational complexity and the need for new skills and tools.</p>
<p>The key to success was:<br />
- <strong>Incremental approach</strong>: Don't try to rewrite everything at once<br />
- <strong>Strong foundation</strong>: Invest in monitoring, logging, and infrastructure<br />
- <strong>Team alignment</strong>: Ensure everyone understands the goals and challenges<br />
- <strong>Patience</strong>: The benefits take time to materialize</p>
<p>Microservices aren't a silver bullet, but when implemented thoughtfully, they can provide significant benefits for growing organizations. The decision to migrate should be based on actual pain points rather than industry trends, and the execution should be gradual and well-planned.</p>
<p>Our API now handles 10x the traffic with better performance and reliability. More importantly, our teams can work independently and deploy frequently, enabling us to respond quickly to business needs. The journey continues as we optimize our services and learn from our experiences in production.</p>
<p>For teams considering a similar migration, my advice is: understand your current constraints, plan thoroughly, invest in tooling and monitoring, and be prepared for the complexity that comes with distributed systems. The benefits are real, but so are the challenges.</p>
                </div>
            </article>
        </div>
    </main>
    
    <footer>
        <p>&copy; 2025 我的博客. All rights reserved.</p>
    </footer>
</body>
</html>