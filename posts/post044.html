<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Database Migration at Scale: Lessons from Zero-Downtime PostgreSQL Upgrade - 我的博客</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1 class="slogan">记录思考，分享生活</h1>
    </header>
    
    <main>
        <div class="container">
            <a href="../index.html" class="back-link">← 返回首页</a>
            
            <article class="article-page">
                <div class="article-header">
                    <h1>Database Migration at Scale: Lessons from Zero-Downtime PostgreSQL Upgrade</h1>
                    <p class="article-date">2024年09月10日</p>
                </div>
                
                <div class="article-content">
                    <hr />
<p>title: "Database Migration at Scale: Lessons from Zero-Downtime PostgreSQL Upgrade"<br />
date: "2024-09-10"<br />
tags: ["database", "migration", "postgresql", "zero-downtime", "infrastructure"]</p>
<hr />
<h1>Database Migration at Scale: Lessons from Zero-Downtime PostgreSQL Upgrade</h1>
<p>Last year, our team faced a daunting challenge: migrating our production PostgreSQL database from version 11 to 14 while serving 50,000+ concurrent users with zero downtime. The database contained 2TB of data across 400+ tables, and any extended outage would cost the company hundreds of thousands of dollars. This is the story of how we successfully executed this migration and the lessons we learned along the way.</p>
<h2>The Challenge: Why Migration Was Critical</h2>
<h3>Database Health Issues</h3>
<p>Our PostgreSQL 11 installation was showing signs of strain:</p>
<pre><code class="language-sql">-- Performance issues we were experiencing
SELECT 
    schemaname,
    tablename,
    attname,
    n_distinct,
    correlation
FROM pg_stats 
WHERE tablename IN ('users', 'orders', 'transactions')
ORDER BY n_distinct DESC;

-- Query performance degradation
EXPLAIN (ANALYZE, BUFFERS) 
SELECT u.*, o.total_amount 
FROM users u 
JOIN orders o ON u.id = o.user_id 
WHERE u.created_at &gt; '2023-01-01' 
AND o.status = 'completed';

-- Result: Seq Scan on users (cost=0.00..892234.84 rows=2847201 width=89)
-- Execution time: 12.847 seconds
</code></pre>
<h3>Key Motivations for Migration</h3>
<pre><code class="language-javascript">const migrationDrivers = {
  performance: {
    currentIssues: [
      'Query performance degradation with data growth',
      'Index bloat causing slow SELECT operations',
      'Vacuum operations taking too long',
      'Connection pooling limitations'
    ],
    expectedImprovements: [
      'Better query planner in PostgreSQL 14',
      'Improved VACUUM performance',
      'Enhanced parallel query execution',
      'Better memory management'
    ]
  },

  features: {
    needed: [
      'Logical replication improvements',
      'Better JSON/JSONB performance', 
      'Enhanced monitoring capabilities',
      'Improved security features'
    ],
    businessValue: [
      'Real-time analytics capabilities',
      'Better audit logging',
      'Compliance requirements',
      'Developer productivity'
    ]
  },

  maintenance: {
    currentChallenges: [
      'Extended maintenance windows',
      'Manual optimization requirements',
      'Limited monitoring visibility',
      'Growing operational complexity'
    ],
    expectedBenefits: [
      'Automated maintenance improvements',
      'Better diagnostic tools',
      'Reduced manual intervention',
      'Enhanced stability'
    ]
  }
};
</code></pre>
<h2>Migration Strategy Planning</h2>
<h3>Risk Assessment and Requirements</h3>
<pre><code class="language-javascript">// Comprehensive risk analysis
const riskAssessment = {
  dataIntegrity: {
    risk: 'HIGH',
    mitigation: [
      'Comprehensive backup strategy',
      'Data validation checksums',
      'Rolling back capability',
      'Real-time replication verification'
    ]
  },

  downtime: {
    risk: 'CRITICAL',
    requirement: 'Zero downtime - business critical applications',
    strategy: 'Blue-green deployment with logical replication'
  },

  performance: {
    risk: 'MEDIUM',
    concerns: [
      'Query plan changes',
      'Application compatibility', 
      'Connection behavior differences',
      'Memory usage patterns'
    ]
  },

  rollback: {
    requirement: 'Complete rollback capability within 30 minutes',
    strategy: 'Maintain source database until verification complete'
  }
};
</code></pre>
<h3>Architecture Design</h3>
<p>We designed a comprehensive migration architecture:</p>
<pre><code class="language-mermaid">graph TD
    A[Application Layer] --&gt; B[PgBouncer Connection Pool]
    B --&gt; C{Traffic Router}
    C --&gt; D[PostgreSQL 11 - Source]
    C --&gt; E[PostgreSQL 14 - Target]
    D --&gt; F[Logical Replication]
    F --&gt; E
    G[Monitoring System] --&gt; D
    G --&gt; E
    H[Backup System] --&gt; D
    I[Backup System] --&gt; E
</code></pre>
<h2>Implementation Phase 1: Infrastructure Setup</h2>
<h3>Setting Up the Target Environment</h3>
<pre><code class="language-bash"># PostgreSQL 14 installation and configuration
#!/bin/bash

# Install PostgreSQL 14
sudo apt-get update
sudo apt-get install -y postgresql-14 postgresql-client-14 postgresql-contrib-14

# Configure PostgreSQL for high performance
cat &gt; /etc/postgresql/14/main/postgresql.conf &lt;&lt; EOF
# Memory settings
shared_buffers = 8GB                    # 25% of RAM
effective_cache_size = 24GB             # 75% of RAM
work_mem = 256MB
maintenance_work_mem = 2GB

# Connection settings
max_connections = 200
superuser_reserved_connections = 3

# WAL settings for replication
wal_level = logical
max_wal_senders = 10
max_replication_slots = 10
max_logical_replication_workers = 4

# Performance settings
random_page_cost = 1.1                 # SSD optimized
effective_io_concurrency = 200
checkpoint_timeout = 15min
checkpoint_completion_target = 0.9
wal_buffers = 16MB

# Logging
log_statement = 'ddl'
log_min_duration_statement = 1000
log_line_prefix = '[%t] %u@%d '

# Other optimizations
autovacuum_max_workers = 6
autovacuum_vacuum_scale_factor = 0.1
autovacuum_analyze_scale_factor = 0.05
EOF

# Configure pg_hba.conf for replication
cat &gt;&gt; /etc/postgresql/14/main/pg_hba.conf &lt;&lt; EOF
# Replication connections
host replication postgres 10.0.0.0/8 md5
host all postgres 10.0.0.0/8 md5
EOF

# Start PostgreSQL 14
sudo systemctl enable postgresql@14-main
sudo systemctl start postgresql@14-main
</code></pre>
<h3>Database Schema Migration</h3>
<pre><code class="language-sql">-- Schema migration script
-- 1. Create target database structure
CREATE DATABASE ecommerce_v14 
WITH OWNER = postgres 
ENCODING = 'UTF8' 
LC_COLLATE = 'en_US.UTF-8' 
LC_CTYPE = 'en_US.UTF-8';

\c ecommerce_v14

-- 2. Create extensions
CREATE EXTENSION IF NOT EXISTS &quot;uuid-ossp&quot;;
CREATE EXTENSION IF NOT EXISTS &quot;pg_trgm&quot;;
CREATE EXTENSION IF NOT EXISTS &quot;btree_gin&quot;;
CREATE EXTENSION IF NOT EXISTS &quot;pg_stat_statements&quot;;

-- 3. Create schemas
CREATE SCHEMA IF NOT EXISTS public;
CREATE SCHEMA IF NOT EXISTS audit;
CREATE SCHEMA IF NOT EXISTS reporting;

-- 4. Migrate table structures (automated script)
-- Generate schema migration from source database
\! pg_dump -h source-db -U postgres -s ecommerce | \
   sed 's/postgresql-11/postgresql-14/g' | \
   psql -h target-db -U postgres ecommerce_v14

-- 5. Verify schema consistency
SELECT 
    t1.table_name,
    t1.column_name,
    t1.data_type,
    CASE 
        WHEN t2.column_name IS NULL THEN 'MISSING'
        WHEN t1.data_type != t2.data_type THEN 'TYPE_MISMATCH'
        ELSE 'OK'
    END as status
FROM information_schema.columns t1
FULL OUTER JOIN information_schema.columns t2 
    ON t1.table_name = t2.table_name 
    AND t1.column_name = t2.column_name
WHERE t1.table_schema = 'public' 
    AND (t2.table_schema = 'public' OR t2.table_schema IS NULL)
ORDER BY t1.table_name, t1.column_name;
</code></pre>
<h2>Implementation Phase 2: Logical Replication Setup</h2>
<h3>Creating Publication and Subscription</h3>
<pre><code class="language-sql">-- On source database (PostgreSQL 11)
-- Create publication for all tables
CREATE PUBLICATION migration_pub FOR ALL TABLES;

-- Verify publication
SELECT pubname, puballtables, pubinsert, pubupdate, pubdelete 
FROM pg_publication;

-- Create replication slot
SELECT pg_create_logical_replication_slot('migration_slot', 'pgoutput');

-- On target database (PostgreSQL 14)
-- Create subscription
CREATE SUBSCRIPTION migration_sub 
CONNECTION 'host=source-db port=5432 user=postgres dbname=ecommerce'
PUBLICATION migration_pub
WITH (copy_data = true, enabled = true);

-- Monitor replication status
SELECT 
    subscription_name,
    pid,
    received_lsn,
    last_msg_send_time,
    last_msg_receipt_time,
    latest_end_lsn,
    latest_end_time
FROM pg_stat_subscription;
</code></pre>
<h3>Monitoring Replication Lag</h3>
<pre><code class="language-python"># Replication monitoring script
import psycopg2
import time
import logging
from datetime import datetime, timedelta

class ReplicationMonitor:
    def __init__(self, source_conn_params, target_conn_params):
        self.source_conn = psycopg2.connect(**source_conn_params)
        self.target_conn = psycopg2.connect(**target_conn_params)
        self.logger = logging.getLogger(__name__)

    def check_replication_lag(self):
        &quot;&quot;&quot;Check replication lag in seconds&quot;&quot;&quot;
        try:
            # Get current LSN from source
            with self.source_conn.cursor() as cursor:
                cursor.execute(&quot;SELECT pg_current_wal_lsn();&quot;)
                source_lsn = cursor.fetchone()[0]

            # Get latest received LSN from target
            with self.target_conn.cursor() as cursor:
                cursor.execute(&quot;&quot;&quot;
                    SELECT 
                        received_lsn,
                        last_msg_receipt_time,
                        EXTRACT(EPOCH FROM (now() - last_msg_receipt_time)) as lag_seconds
                    FROM pg_stat_subscription 
                    WHERE subscription_name = 'migration_sub';
                &quot;&quot;&quot;)
                result = cursor.fetchone()

                if result:
                    received_lsn, last_receipt, lag_seconds = result
                    return {
                        'source_lsn': source_lsn,
                        'received_lsn': received_lsn,
                        'lag_seconds': lag_seconds,
                        'last_receipt': last_receipt
                    }

        except Exception as e:
            self.logger.error(f&quot;Error checking replication lag: {e}&quot;)
            return None

    def verify_data_consistency(self, table_name):
        &quot;&quot;&quot;Verify data consistency between source and target&quot;&quot;&quot;
        try:
            # Count rows in source
            with self.source_conn.cursor() as cursor:
                cursor.execute(f&quot;SELECT COUNT(*) FROM {table_name};&quot;)
                source_count = cursor.fetchone()[0]

            # Count rows in target  
            with self.target_conn.cursor() as cursor:
                cursor.execute(f&quot;SELECT COUNT(*) FROM {table_name};&quot;)
                target_count = cursor.fetchone()[0]

            # Calculate checksum for sample data
            checksum_query = f&quot;&quot;&quot;
                SELECT MD5(
                    string_agg(
                        MD5(CAST((t.*)::text AS bytea))::text, 
                        '' ORDER BY id
                    )
                ) FROM (
                    SELECT * FROM {table_name} 
                    ORDER BY id 
                    LIMIT 1000
                ) t;
            &quot;&quot;&quot;

            with self.source_conn.cursor() as cursor:
                cursor.execute(checksum_query)
                source_checksum = cursor.fetchone()[0]

            with self.target_conn.cursor() as cursor:
                cursor.execute(checksum_query)
                target_checksum = cursor.fetchone()[0]

            return {
                'table': table_name,
                'source_count': source_count,
                'target_count': target_count,
                'count_match': source_count == target_count,
                'checksum_match': source_checksum == target_checksum,
                'source_checksum': source_checksum,
                'target_checksum': target_checksum
            }

        except Exception as e:
            self.logger.error(f&quot;Error verifying consistency for {table_name}: {e}&quot;)
            return None

    def monitor_replication(self, check_interval=30):
        &quot;&quot;&quot;Continuously monitor replication&quot;&quot;&quot;
        while True:
            lag_info = self.check_replication_lag()

            if lag_info:
                self.logger.info(f&quot;Replication lag: {lag_info['lag_seconds']:.2f} seconds&quot;)

                # Alert if lag is too high
                if lag_info['lag_seconds'] &gt; 60:  # 1 minute
                    self.logger.warning(f&quot;High replication lag detected: {lag_info['lag_seconds']} seconds&quot;)
                    # Send alert to monitoring system

            time.sleep(check_interval)

# Usage
if __name__ == &quot;__main__&quot;:
    source_params = {
        'host': 'source-db',
        'database': 'ecommerce',
        'user': 'postgres',
        'password': 'password'
    }

    target_params = {
        'host': 'target-db', 
        'database': 'ecommerce_v14',
        'user': 'postgres',
        'password': 'password'
    }

    monitor = ReplicationMonitor(source_params, target_params)
    monitor.monitor_replication()
</code></pre>
<h2>Implementation Phase 3: Application Testing</h2>
<h3>Compatibility Testing Framework</h3>
<pre><code class="language-python"># Database compatibility testing
import asyncio
import asyncpg
import pytest
from typing import List, Dict, Any
import time

class DatabaseCompatibilityTester:
    def __init__(self, source_dsn: str, target_dsn: str):
        self.source_dsn = source_dsn
        self.target_dsn = target_dsn

    async def setup_connections(self):
        self.source_pool = await asyncpg.create_pool(self.source_dsn, min_size=5, max_size=20)
        self.target_pool = await asyncpg.create_pool(self.target_dsn, min_size=5, max_size=20)

    async def test_query_compatibility(self, queries: List[str]) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Test if queries produce same results on both databases&quot;&quot;&quot;
        results = {}

        for query in queries:
            try:
                # Execute on source
                async with self.source_pool.acquire() as source_conn:
                    start_time = time.time()
                    source_result = await source_conn.fetch(query)
                    source_duration = time.time() - start_time

                # Execute on target
                async with self.target_pool.acquire() as target_conn:
                    start_time = time.time()
                    target_result = await target_conn.fetch(query)
                    target_duration = time.time() - start_time

                # Compare results
                results[query] = {
                    'source_rows': len(source_result),
                    'target_rows': len(target_result),
                    'source_duration': source_duration,
                    'target_duration': target_duration,
                    'performance_improvement': (source_duration - target_duration) / source_duration * 100,
                    'results_match': source_result == target_result,
                    'status': 'PASS' if source_result == target_result else 'FAIL'
                }

            except Exception as e:
                results[query] = {
                    'status': 'ERROR',
                    'error': str(e)
                }

        return results

    async def test_write_operations(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Test write operations on target database&quot;&quot;&quot;
        test_results = {}

        test_operations = [
            {
                'name': 'INSERT_SINGLE',
                'query': &quot;INSERT INTO test_table (name, email) VALUES ($1, $2) RETURNING id&quot;,
                'params': ['Test User', 'test@example.com']
            },
            {
                'name': 'UPDATE_SINGLE', 
                'query': &quot;UPDATE test_table SET name = $1 WHERE email = $2&quot;,
                'params': ['Updated User', 'test@example.com']
            },
            {
                'name': 'DELETE_SINGLE',
                'query': &quot;DELETE FROM test_table WHERE email = $1&quot;,
                'params': ['test@example.com']
            }
        ]

        for operation in test_operations:
            try:
                async with self.target_pool.acquire() as conn:
                    async with conn.transaction():
                        start_time = time.time()

                        if 'RETURNING' in operation['query']:
                            result = await conn.fetchval(operation['query'], *operation['params'])
                        else:
                            result = await conn.execute(operation['query'], *operation['params'])

                        duration = time.time() - start_time

                        test_results[operation['name']] = {
                            'status': 'PASS',
                            'duration': duration,
                            'result': result
                        }

                        # Rollback test transaction
                        raise Exception(&quot;Rollback test transaction&quot;)

            except Exception as e:
                if &quot;Rollback test transaction&quot; not in str(e):
                    test_results[operation['name']] = {
                        'status': 'FAIL',
                        'error': str(e)
                    }
                else:
                    # Expected rollback
                    test_results[operation['name']]['status'] = 'PASS'

        return test_results

    async def performance_benchmark(self, concurrent_users: int = 50) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Run performance benchmark comparing source and target&quot;&quot;&quot;

        benchmark_queries = [
            &quot;SELECT COUNT(*) FROM users WHERE created_at &gt; NOW() - INTERVAL '30 days'&quot;,
            &quot;SELECT u.*, COUNT(o.id) as order_count FROM users u LEFT JOIN orders o ON u.id = o.user_id GROUP BY u.id LIMIT 100&quot;,
            &quot;SELECT * FROM products WHERE name ILIKE '%laptop%' ORDER BY price DESC LIMIT 20&quot;
        ]

        results = {}

        for query in benchmark_queries:
            # Benchmark source database
            source_times = []
            tasks = []

            for _ in range(concurrent_users):
                task = self._execute_timed_query(self.source_pool, query)
                tasks.append(task)

            source_results = await asyncio.gather(*tasks)
            source_times = [r['duration'] for r in source_results if r['success']]

            # Benchmark target database
            target_times = []
            tasks = []

            for _ in range(concurrent_users):
                task = self._execute_timed_query(self.target_pool, query)
                tasks.append(task)

            target_results = await asyncio.gather(*tasks)
            target_times = [r['duration'] for r in target_results if r['success']]

            results[query] = {
                'source_avg': sum(source_times) / len(source_times) if source_times else 0,
                'target_avg': sum(target_times) / len(target_times) if target_times else 0,
                'source_min': min(source_times) if source_times else 0,
                'target_min': min(target_times) if target_times else 0,
                'source_max': max(source_times) if source_times else 0,
                'target_max': max(target_times) if target_times else 0,
                'improvement_pct': 0
            }

            if results[query]['source_avg'] &gt; 0:
                improvement = (results[query]['source_avg'] - results[query]['target_avg']) / results[query]['source_avg'] * 100
                results[query]['improvement_pct'] = improvement

        return results

    async def _execute_timed_query(self, pool, query):
        &quot;&quot;&quot;Execute query and measure execution time&quot;&quot;&quot;
        try:
            async with pool.acquire() as conn:
                start_time = time.time()
                await conn.fetch(query)
                duration = time.time() - start_time
                return {'success': True, 'duration': duration}
        except Exception as e:
            return {'success': False, 'error': str(e)}

# Test runner
@pytest.mark.asyncio
async def test_migration_compatibility():
    tester = DatabaseCompatibilityTester(
        &quot;postgresql://postgres:password@source-db/ecommerce&quot;,
        &quot;postgresql://postgres:password@target-db/ecommerce_v14&quot;
    )

    await tester.setup_connections()

    # Test critical queries
    critical_queries = [
        &quot;SELECT COUNT(*) FROM users&quot;,
        &quot;SELECT COUNT(*) FROM orders WHERE status = 'completed'&quot;,
        &quot;SELECT SUM(total_amount) FROM orders WHERE created_at &gt; NOW() - INTERVAL '1 day'&quot;
    ]

    query_results = await tester.test_query_compatibility(critical_queries)

    # Verify all tests pass
    for query, result in query_results.items():
        assert result['status'] == 'PASS', f&quot;Query failed: {query} - {result}&quot;
        assert result['results_match'], f&quot;Results don't match for: {query}&quot;

    # Test write operations
    write_results = await tester.test_write_operations()
    for operation, result in write_results.items():
        assert result['status'] == 'PASS', f&quot;Write operation failed: {operation}&quot;

    print(&quot;All compatibility tests passed!&quot;)
</code></pre>
<h2>Implementation Phase 4: Traffic Switching</h2>
<h3>Blue-Green Deployment with PgBouncer</h3>
<pre><code class="language-bash"># PgBouncer configuration for seamless switching
cat &gt; /etc/pgbouncer/pgbouncer.ini &lt;&lt; EOF
[databases]
ecommerce = host=source-db port=5432 dbname=ecommerce
ecommerce_new = host=target-db port=5432 dbname=ecommerce_v14

[pgbouncer]
listen_port = 6432
listen_addr = *
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
pool_mode = transaction
server_reset_query = DISCARD ALL
max_client_conn = 1000
default_pool_size = 50
reserve_pool_size = 5
server_lifetime = 3600
server_idle_timeout = 600
log_connections = 1
log_disconnections = 1
stats_period = 60
EOF

# Create user authentication file
cat &gt; /etc/pgbouncer/userlist.txt &lt;&lt; EOF
&quot;postgres&quot; &quot;md5hash_of_password&quot;
&quot;app_user&quot; &quot;md5hash_of_app_password&quot;
EOF

# Start PgBouncer
sudo systemctl enable pgbouncer
sudo systemctl start pgbouncer
</code></pre>
<h3>Gradual Traffic Migration Script</h3>
<pre><code class="language-python"># Gradual traffic migration controller
import time
import logging
import psycopg2
from typing import Dict, List
import subprocess

class TrafficMigrationController:
    def __init__(self, pgbouncer_host: str, admin_user: str, admin_password: str):
        self.pgbouncer_host = pgbouncer_host
        self.admin_user = admin_user
        self.admin_password = admin_password
        self.logger = logging.getLogger(__name__)

    def get_pgbouncer_admin_connection(self):
        &quot;&quot;&quot;Get connection to PgBouncer admin console&quot;&quot;&quot;
        return psycopg2.connect(
            host=self.pgbouncer_host,
            port=6432,
            database='pgbouncer',
            user=self.admin_user,
            password=self.admin_password
        )

    def get_connection_stats(self) -&gt; Dict:
        &quot;&quot;&quot;Get current connection statistics&quot;&quot;&quot;
        with self.get_pgbouncer_admin_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(&quot;SHOW POOLS;&quot;)
                pools = cursor.fetchall()

                cursor.execute(&quot;SHOW STATS;&quot;)
                stats = cursor.fetchall()

                return {
                    'pools': pools,
                    'stats': stats
                }

    def migrate_traffic_gradually(self, steps: List[Dict]):
        &quot;&quot;&quot;Migrate traffic in gradual steps&quot;&quot;&quot;

        for step_num, step in enumerate(steps, 1):
            self.logger.info(f&quot;Starting migration step {step_num}/{len(steps)}&quot;)
            self.logger.info(f&quot;Target: {step['percentage']}% to new database&quot;)

            try:
                # Update PgBouncer configuration
                self._update_pgbouncer_config(step['percentage'])

                # Wait for connections to migrate
                time.sleep(step['wait_time'])

                # Monitor for issues
                health_check = self._health_check(step['check_duration'])

                if not health_check['healthy']:
                    self.logger.error(f&quot;Health check failed at step {step_num}&quot;)
                    self._rollback_step()
                    return False

                self.logger.info(f&quot;Step {step_num} completed successfully&quot;)

            except Exception as e:
                self.logger.error(f&quot;Error in step {step_num}: {e}&quot;)
                self._rollback_step()
                return False

        self.logger.info(&quot;Traffic migration completed successfully!&quot;)
        return True

    def _update_pgbouncer_config(self, percentage: int):
        &quot;&quot;&quot;Update PgBouncer to route traffic percentage to new database&quot;&quot;&quot;

        # For simplicity, we'll use connection pool allocation
        # In production, you might use connection-level routing

        total_connections = 100
        new_db_connections = int(total_connections * percentage / 100)
        old_db_connections = total_connections - new_db_connections

        config_template = f&quot;&quot;&quot;
[databases]
ecommerce = host=source-db port=5432 dbname=ecommerce pool_size={old_db_connections}
ecommerce_new = host=target-db port=5432 dbname=ecommerce_v14 pool_size={new_db_connections}

[pgbouncer]
listen_port = 6432
listen_addr = *
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
pool_mode = transaction
server_reset_query = DISCARD ALL
max_client_conn = 1000
default_pool_size = 50
&quot;&quot;&quot;

        # Write new configuration
        with open('/etc/pgbouncer/pgbouncer.ini.new', 'w') as f:
            f.write(config_template)

        # Reload PgBouncer configuration
        subprocess.run(['sudo', 'systemctl', 'reload', 'pgbouncer'], check=True)

    def _health_check(self, duration: int) -&gt; Dict:
        &quot;&quot;&quot;Perform health check for specified duration&quot;&quot;&quot;

        start_time = time.time()
        error_count = 0
        total_checks = 0

        while time.time() - start_time &lt; duration:
            try:
                # Check database connectivity
                self._check_database_connectivity()

                # Check replication lag
                lag = self._check_replication_lag()
                if lag &gt; 10:  # 10 seconds threshold
                    error_count += 1

                # Check error rates from application logs
                error_rate = self._check_application_error_rate()
                if error_rate &gt; 0.05:  # 5% threshold
                    error_count += 1

                total_checks += 1
                time.sleep(5)  # Check every 5 seconds

            except Exception as e:
                self.logger.error(f&quot;Health check error: {e}&quot;)
                error_count += 1
                total_checks += 1

        error_rate = error_count / total_checks if total_checks &gt; 0 else 1

        return {
            'healthy': error_rate &lt; 0.1,  # Less than 10% error rate
            'error_rate': error_rate,
            'total_checks': total_checks,
            'error_count': error_count
        }

    def _check_database_connectivity(self):
        &quot;&quot;&quot;Check if both databases are accessible&quot;&quot;&quot;
        # Check source database
        conn = psycopg2.connect(
            host='source-db',
            database='ecommerce',
            user='postgres',
            password='password'
        )
        conn.close()

        # Check target database
        conn = psycopg2.connect(
            host='target-db',
            database='ecommerce_v14',
            user='postgres', 
            password='password'
        )
        conn.close()

    def _check_replication_lag(self) -&gt; float:
        &quot;&quot;&quot;Check replication lag in seconds&quot;&quot;&quot;
        conn = psycopg2.connect(
            host='target-db',
            database='ecommerce_v14',
            user='postgres',
            password='password'
        )

        with conn.cursor() as cursor:
            cursor.execute(&quot;&quot;&quot;
                SELECT EXTRACT(EPOCH FROM (now() - last_msg_receipt_time)) as lag_seconds
                FROM pg_stat_subscription 
                WHERE subscription_name = 'migration_sub';
            &quot;&quot;&quot;)
            result = cursor.fetchone()

        conn.close()
        return result[0] if result and result[0] else 0

    def _check_application_error_rate(self) -&gt; float:
        &quot;&quot;&quot;Check application error rate from logs&quot;&quot;&quot;
        # This would integrate with your logging system
        # For now, return a mock value
        return 0.01  # 1% error rate

    def _rollback_step(self):
        &quot;&quot;&quot;Rollback to previous configuration&quot;&quot;&quot;
        self.logger.info(&quot;Rolling back migration step...&quot;)

        # Restore original PgBouncer configuration
        subprocess.run(['sudo', 'cp', '/etc/pgbouncer/pgbouncer.ini.backup', '/etc/pgbouncer/pgbouncer.ini'])
        subprocess.run(['sudo', 'systemctl', 'reload', 'pgbouncer'])

# Migration execution
if __name__ == &quot;__main__&quot;:
    controller = TrafficMigrationController(
        pgbouncer_host='pgbouncer-host',
        admin_user='postgres',
        admin_password='password'
    )

    # Define migration steps
    migration_steps = [
        {'percentage': 10, 'wait_time': 300, 'check_duration': 600},   # 10% for 10 minutes
        {'percentage': 25, 'wait_time': 300, 'check_duration': 600},   # 25% for 10 minutes
        {'percentage': 50, 'wait_time': 600, 'check_duration': 900},   # 50% for 15 minutes
        {'percentage': 75, 'wait_time': 600, 'check_duration': 900},   # 75% for 15 minutes
        {'percentage': 100, 'wait_time': 300, 'check_duration': 1800}  # 100% for 30 minutes
    ]

    success = controller.migrate_traffic_gradually(migration_steps)

    if success:
        print(&quot;Migration completed successfully!&quot;)
    else:
        print(&quot;Migration failed - rolled back to original configuration&quot;)
</code></pre>
<h2>Post-Migration Verification and Optimization</h2>
<h3>Comprehensive Data Validation</h3>
<p>The migration implementation utilized sophisticated spatiotemporal modeling techniques to understand data access patterns during the transition period, implemented lightweight engines for efficient real-time validation, and created multi-modal data integration systems that verified consistency across different data types and structures.</p>
<pre><code class="language-python"># Comprehensive post-migration validation
import asyncio
import asyncpg
import hashlib
from typing import Dict, List, Any
import concurrent.futures

class PostMigrationValidator:
    def __init__(self, source_dsn: str, target_dsn: str):
        self.source_dsn = source_dsn
        self.target_dsn = target_dsn

    async def full_data_validation(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Perform comprehensive data validation&quot;&quot;&quot;

        validation_results = {
            'table_validation': await self._validate_all_tables(),
            'referential_integrity': await self._check_referential_integrity(),
            'index_validation': await self._validate_indexes(),
            'sequence_validation': await self._validate_sequences(),
            'constraint_validation': await self._validate_constraints()
        }

        return validation_results

    async def _validate_all_tables(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Validate data consistency for all tables&quot;&quot;&quot;

        # Get list of all tables
        async with asyncpg.connect(self.source_dsn) as conn:
            tables = await conn.fetch(&quot;&quot;&quot;
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_type = 'BASE TABLE'
                ORDER BY table_name;
            &quot;&quot;&quot;)

        table_names = [row['table_name'] for row in tables]

        # Validate tables in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            loop = asyncio.get_event_loop()
            tasks = [
                loop.run_in_executor(executor, self._validate_table_async, table_name)
                for table_name in table_names
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

        validation_results = {}
        for table_name, result in zip(table_names, results):
            if isinstance(result, Exception):
                validation_results[table_name] = {'status': 'ERROR', 'error': str(result)}
            else:
                validation_results[table_name] = result

        return validation_results

    def _validate_table_async(self, table_name: str) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Validate a single table (runs in thread executor)&quot;&quot;&quot;
        return asyncio.run(self._validate_table(table_name))

    async def _validate_table(self, table_name: str) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Validate data consistency for a single table&quot;&quot;&quot;

        # Connect to both databases
        source_conn = await asyncpg.connect(self.source_dsn)
        target_conn = await asyncpg.connect(self.target_dsn)

        try:
            # Get row counts
            source_count = await source_conn.fetchval(f&quot;SELECT COUNT(*) FROM {table_name}&quot;)
            target_count = await target_conn.fetchval(f&quot;SELECT COUNT(*) FROM {table_name}&quot;)

            # Get checksums for sample data
            sample_size = min(10000, source_count)

            checksum_query = f&quot;&quot;&quot;
                SELECT MD5(
                    string_agg(
                        MD5(CAST(ROW({table_name}.*)::text AS bytea))::text,
                        '' ORDER BY (
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_name = '{table_name}' 
                            AND table_schema = 'public'
                            ORDER BY ordinal_position 
                            LIMIT 1
                        )
                    )
                ) FROM (
                    SELECT * FROM {table_name} 
                    ORDER BY (
                        SELECT column_name 
                        FROM information_schema.columns 
                        WHERE table_name = '{table_name}' 
                        AND table_schema = 'public'
                        ORDER BY ordinal_position 
                        LIMIT 1
                    )
                    LIMIT {sample_size}
                ) {table_name};
            &quot;&quot;&quot;

            source_checksum = await source_conn.fetchval(checksum_query)
            target_checksum = await target_conn.fetchval(checksum_query)

            # Additional validation for critical tables
            additional_checks = {}
            if table_name in ['users', 'orders', 'transactions']:
                additional_checks = await self._detailed_table_validation(
                    source_conn, target_conn, table_name
                )

            return {
                'status': 'PASS' if source_count == target_count and source_checksum == target_checksum else 'FAIL',
                'source_count': source_count,
                'target_count': target_count,
                'count_match': source_count == target_count,
                'checksum_match': source_checksum == target_checksum,
                'source_checksum': source_checksum,
                'target_checksum': target_checksum,
                'additional_checks': additional_checks
            }

        finally:
            await source_conn.close()
            await target_conn.close()

    async def _detailed_table_validation(self, source_conn, target_conn, table_name: str) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Perform detailed validation for critical tables&quot;&quot;&quot;

        checks = {}

        if table_name == 'users':
            # Check unique email constraints
            source_unique_emails = await source_conn.fetchval(
                &quot;SELECT COUNT(DISTINCT email) FROM users&quot;
            )
            target_unique_emails = await target_conn.fetchval(
                &quot;SELECT COUNT(DISTINCT email) FROM users&quot;
            )
            checks['unique_emails_match'] = source_unique_emails == target_unique_emails

        elif table_name == 'orders':
            # Check order totals
            source_total = await source_conn.fetchval(
                &quot;SELECT SUM(total_amount) FROM orders WHERE status = 'completed'&quot;
            )
            target_total = await target_conn.fetchval(
                &quot;SELECT SUM(total_amount) FROM orders WHERE status = 'completed'&quot;
            )
            checks['order_totals_match'] = abs(float(source_total or 0) - float(target_total or 0)) &lt; 0.01

        elif table_name == 'transactions':
            # Check transaction balance
            source_balance = await source_conn.fetchval(
                &quot;SELECT SUM(amount) FROM transactions&quot;
            )
            target_balance = await target_conn.fetchval(
                &quot;SELECT SUM(amount) FROM transactions&quot;
            )
            checks['transaction_balance_match'] = abs(float(source_balance or 0) - float(target_balance or 0)) &lt; 0.01

        return checks

    async def _check_referential_integrity(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Check referential integrity constraints&quot;&quot;&quot;

        async with asyncpg.connect(self.target_dsn) as conn:
            # Get all foreign key constraints
            fk_constraints = await conn.fetch(&quot;&quot;&quot;
                SELECT 
                    tc.table_name,
                    tc.constraint_name,
                    kcu.column_name,
                    ccu.table_name AS foreign_table_name,
                    ccu.column_name AS foreign_column_name
                FROM information_schema.table_constraints AS tc
                JOIN information_schema.key_column_usage AS kcu
                    ON tc.constraint_name = kcu.constraint_name
                    AND tc.table_schema = kcu.table_schema
                JOIN information_schema.constraint_column_usage AS ccu
                    ON ccu.constraint_name = tc.constraint_name
                    AND ccu.table_schema = tc.table_schema
                WHERE tc.constraint_type = 'FOREIGN KEY'
                AND tc.table_schema = 'public';
            &quot;&quot;&quot;)

            integrity_results = {}

            for constraint in fk_constraints:
                table_name = constraint['table_name']
                column_name = constraint['column_name'] 
                foreign_table = constraint['foreign_table_name']
                foreign_column = constraint['foreign_column_name']

                # Check for orphaned records
                orphaned_count = await conn.fetchval(f&quot;&quot;&quot;
                    SELECT COUNT(*) 
                    FROM {table_name} t1
                    LEFT JOIN {foreign_table} t2 ON t1.{column_name} = t2.{foreign_column}
                    WHERE t1.{column_name} IS NOT NULL 
                    AND t2.{foreign_column} IS NULL;
                &quot;&quot;&quot;)

                constraint_name = constraint['constraint_name']
                integrity_results[constraint_name] = {
                    'table': table_name,
                    'column': column_name,
                    'foreign_table': foreign_table,
                    'foreign_column': foreign_column,
                    'orphaned_records': orphaned_count,
                    'status': 'PASS' if orphaned_count == 0 else 'FAIL'
                }

        return integrity_results

# Performance comparison after migration
class PerformanceComparison:
    def __init__(self, source_dsn: str, target_dsn: str):
        self.source_dsn = source_dsn
        self.target_dsn = target_dsn

    async def compare_query_performance(self, queries: List[str]) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Compare query performance between source and target&quot;&quot;&quot;

        results = {}

        for query in queries:
            source_time = await self._measure_query_time(self.source_dsn, query)
            target_time = await self._measure_query_time(self.target_dsn, query)

            improvement = ((source_time - target_time) / source_time * 100) if source_time &gt; 0 else 0

            results[query] = {
                'source_time': source_time,
                'target_time': target_time,
                'improvement_percent': improvement,
                'status': 'IMPROVED' if improvement &gt; 0 else 'DEGRADED'
            }

        return results

    async def _measure_query_time(self, dsn: str, query: str) -&gt; float:
        &quot;&quot;&quot;Measure query execution time&quot;&quot;&quot;

        async with asyncpg.connect(dsn) as conn:
            start_time = time.time()
            await conn.fetch(query)
            return time.time() - start_time
</code></pre>
<h2>Results and Lessons Learned</h2>
<h3>Migration Success Metrics</h3>
<pre><code class="language-javascript">const migrationResults = {
  downtime: {
    planned: '0 minutes',
    actual: '0 minutes',
    status: 'SUCCESS'
  },

  performance: {
    queryPerformance: {
      improvement: '35%',
      p95Latency: {
        before: '2.3s',
        after: '1.5s'
      }
    },
    throughput: {
      improvement: '25%',
      qps: {
        before: '8,500',
        after: '10,600'
      }
    }
  },

  dataIntegrity: {
    recordsValidated: '847,392,847',
    consistencyChecks: 'PASSED',
    checksumValidation: 'PASSED',
    referentialIntegrity: 'PASSED'
  },

  businessImpact: {
    revenueImpact: '$0 (zero downtime)',
    customerSatisfaction: 'No complaints',
    systemStability: 'Improved',
    maintenanceEfficiency: '+40%'
  }
};
</code></pre>
<h3>Key Lessons Learned</h3>
<ol>
<li><strong>Preparation is Everything</strong>: We spent 6 weeks in preparation for 1 day of execution</li>
<li><strong>Monitoring is Critical</strong>: Real-time monitoring of both technical and business metrics</li>
<li><strong>Gradual Migration</strong>: Step-by-step traffic migration allowed us to detect and fix issues early</li>
<li><strong>Rollback Strategy</strong>: Having a clear rollback plan gave confidence to proceed</li>
<li><strong>Team Coordination</strong>: Clear communication and defined roles were essential</li>
</ol>
<h3>Challenges Overcome</h3>
<pre><code class="language-javascript">const challengesAndSolutions = {
  replicationLag: {
    challenge: 'Logical replication lag during high traffic',
    solution: 'Optimized network bandwidth and parallel workers',
    impact: 'Reduced lag from 30s to &lt;5s'
  },

  applicationCompatibility: {
    challenge: 'Some queries behaved differently in PostgreSQL 14',
    solution: 'Comprehensive testing and query optimization',
    impact: 'Fixed 12 compatibility issues before migration'
  },

  connectionPooling: {
    challenge: 'PgBouncer configuration for dual-database setup',
    solution: 'Custom configuration with dynamic routing',
    impact: 'Seamless traffic switching without connection drops'
  },

  dataValidation: {
    challenge: 'Validating 2TB of data consistency',
    solution: 'Parallel validation with sampling and checksums',
    impact: 'Completed validation in 4 hours instead of days'
  }
};
</code></pre>
<h2>Future Recommendations</h2>
<p>Based on our experience, here are recommendations for similar migrations:</p>
<h3>Technical Recommendations</h3>
<pre><code class="language-javascript">const technicalRecommendations = {
  planning: [
    'Start planning at least 3 months in advance',
    'Create detailed migration runbooks',
    'Establish clear success criteria',
    'Plan for multiple rehearsals in staging'
  ],

  tooling: [
    'Invest in automated testing frameworks',
    'Use logical replication for near-zero downtime',
    'Implement comprehensive monitoring',
    'Use connection pooling for traffic switching'
  ],

  validation: [
    'Implement checksum-based data validation',
    'Use parallel processing for large datasets',
    'Validate business logic, not just data',
    'Test referential integrity thoroughly'
  ],

  rollback: [
    'Always have a tested rollback procedure',
    'Maintain parallel systems during migration',
    'Set clear rollback trigger criteria',
    'Practice rollback procedures'
  ]
};
</code></pre>
<h3>Process Recommendations</h3>
<pre><code class="language-javascript">const processRecommendations = {
  teamStructure: [
    'Dedicated migration team with clear roles',
    'Database experts, application developers, and SREs',
    'Clear escalation procedures',
    '24/7 support during migration window'
  ],

  communication: [
    'Regular stakeholder updates',
    'Clear migration timeline communication',
    'Real-time status dashboard',
    'Post-migration success communication'
  ],

  riskManagement: [
    'Comprehensive risk assessment',
    'Risk mitigation strategies',
    'Regular risk review meetings',
    'Contingency planning for edge cases'
  ]
};
</code></pre>
<h2>Conclusion</h2>
<p>Our zero-downtime PostgreSQL migration was a success because of meticulous planning, comprehensive testing, and careful execution. The key was treating it not just as a technical challenge, but as a business-critical operation requiring coordination across multiple teams.</p>
<p>The 35% performance improvement and enhanced features of PostgreSQL 14 have provided significant value to our business, while the zero-downtime approach ensured no impact on our customers. The experience taught us that with proper planning and execution, even complex database migrations can be performed safely in production environments.</p>
<p>For teams considering similar migrations, remember that the technology is just one part of the equation. Success requires equal attention to planning, testing, monitoring, and team coordination. Start early, test thoroughly, and always have a rollback plan.</p>
                </div>
            </article>
        </div>
    </main>
    
    <footer>
        <p>&copy; 2025 我的博客. All rights reserved.</p>
    </footer>
</body>
</html>