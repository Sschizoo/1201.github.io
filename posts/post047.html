<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>容器化部署实践：从Docker到Kubernetes的演进之路 - 我的博客</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1 class="slogan">记录思考，分享生活</h1>
    </header>
    
    <main>
        <div class="container">
            <a href="../index.html" class="back-link">← 返回首页</a>
            
            <article class="article-page">
                <div class="article-header">
                    <h1>容器化部署实践：从Docker到Kubernetes的演进之路</h1>
                    <p class="article-date">2024年10月18日</p>
                </div>
                
                <div class="article-content">
                    <hr />
<p>title: "容器化部署实践：从Docker到Kubernetes的演进之路"<br />
date: "2024-10-18"<br />
tags: ["容器化", "Docker", "Kubernetes", "DevOps", "云原生"]</p>
<hr />
<h1>容器化部署实践：从Docker到Kubernetes的演进之路</h1>
<p>三年前，我们的应用还运行在传统的虚拟机上，部署一次需要2小时，回滚更是让人头疼的操作。如今，通过容器化和Kubernetes编排，我们实现了秒级扩容、蓝绿部署、自动回滚等云原生能力。这篇文章将分享我们从单体应用到云原生架构的完整演进过程。</p>
<h2>传统部署方式的痛点</h2>
<h3>部署环境不一致</h3>
<p>最初我们的部署流程是这样的：</p>
<pre><code class="language-bash"># 传统部署脚本 - 问题重重
#!/bin/bash

echo &quot;开始部署应用...&quot;

# 停止旧服务
sudo systemctl stop myapp

# 备份旧版本（经常忘记）
# sudo cp -r /opt/myapp /opt/myapp.backup.$(date +%Y%m%d)

# 更新应用文件
sudo rsync -av --delete /tmp/build/ /opt/myapp/

# 更新依赖（Python版本不一致问题）
cd /opt/myapp
sudo pip install -r requirements.txt

# 更新配置文件（每个环境都不同）
sudo cp /opt/configs/prod/config.py /opt/myapp/

# 重启服务
sudo systemctl start myapp

# 希望一切正常...
echo &quot;部署完成，请手动检查服务状态&quot;
</code></pre>
<h3>遇到的主要问题</h3>
<pre><code class="language-javascript">const deploymentProblems = {
  environmentInconsistency: {
    issue: '开发、测试、生产环境差异巨大',
    examples: [
      'Python版本不同（3.7 vs 3.8 vs 3.9）',
      '依赖库版本冲突',
      '操作系统补丁差异',
      '环境变量配置不一致'
    ],
    impact: '经常出现&quot;在我机器上能运行&quot;的问题'
  },

  scalabilityIssues: {
    issue: '扩容和缩容困难',
    problems: [
      '手动创建虚拟机需要30分钟',
      '负载均衡器配置复杂',
      '监控和日志收集配置繁琐',
      '资源利用率低（平均CPU使用率仅15%）'
    ]
  },

  deploymentRisks: {
    issue: '部署风险高、回滚困难',
    scenarios: [
      '部署失败导致服务长时间不可用',
      '回滚需要30分钟以上',
      '数据库迁移失败难以恢复',
      '配置错误导致安全漏洞'
    ]
  },

  operationalComplexity: {
    issue: '运维复杂度高',
    challenges: [
      '每台服务器需要单独配置',
      '日志分散在多台机器上',
      '监控指标收集困难',
      '故障排查时间长'
    ]
  }
};
</code></pre>
<h2>Docker容器化改造</h2>
<h3>第一步：编写Dockerfile</h3>
<pre><code class="language-dockerfile"># 多阶段构建优化镜像大小
FROM node:16-alpine AS builder

WORKDIR /app

# 复制package文件，利用Docker层缓存
COPY package*.json ./
RUN npm ci --only=production &amp;&amp; npm cache clean --force

# 复制源代码并构建
COPY . .
RUN npm run build

# 生产环境镜像
FROM node:16-alpine AS production

# 创建非root用户提升安全性
RUN addgroup -g 1001 -S nodejs
RUN adduser -S nextjs -u 1001

WORKDIR /app

# 复制构建产物和依赖
COPY --from=builder /app/dist ./dist
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/package.json ./package.json

# 设置用户权限
USER nextjs

# 暴露端口
EXPOSE 3000

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:3000/health || exit 1

# 启动命令
CMD [&quot;npm&quot;, &quot;start&quot;]
</code></pre>
<h3>Docker镜像优化策略</h3>
<pre><code class="language-dockerfile"># 优化技巧总结

# 1. 使用.dockerignore减少构建上下文
# .dockerignore 文件内容：
node_modules
npm-debug.log
.git
.gitignore
README.md
.env
coverage/
.nyc_output

# 2. 多阶段构建减少镜像大小
FROM golang:1.19-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main .

FROM alpine:latest
RUN apk --no-cache add ca-certificates
WORKDIR /root/
COPY --from=builder /app/main .
CMD [&quot;./main&quot;]

# 3. 优化层缓存策略
# 频繁变动的内容放在后面
COPY package*.json ./
RUN npm install
COPY . .

# 4. 使用特定版本标签而非latest
FROM node:16.14.2-alpine
# 而不是 FROM node:latest

# 5. 合并RUN指令减少层数
RUN apt-get update &amp;&amp; \
    apt-get install -y \
        curl \
        vim \
        git &amp;&amp; \
    apt-get clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/*
</code></pre>
<h3>容器化部署脚本</h3>
<pre><code class="language-bash">#!/bin/bash
# docker-deploy.sh

set -e

APP_NAME=&quot;myapp&quot;
IMAGE_TAG=&quot;${APP_NAME}:$(git rev-parse --short HEAD)&quot;
CONTAINER_NAME=&quot;${APP_NAME}-$(date +%Y%m%d%H%M%S)&quot;

echo &quot;开始构建镜像: ${IMAGE_TAG}&quot;

# 构建镜像
docker build -t ${IMAGE_TAG} .

# 运行健康检查
echo &quot;验证镜像健康性...&quot;
docker run --rm ${IMAGE_TAG} npm test

# 停止旧容器（如果存在）
if docker ps -q -f name=${APP_NAME} | grep -q .; then
    echo &quot;停止旧容器...&quot;
    docker stop ${APP_NAME}
    docker rm ${APP_NAME}
fi

# 启动新容器
echo &quot;启动新容器: ${CONTAINER_NAME}&quot;
docker run -d \
    --name ${APP_NAME} \
    --restart unless-stopped \
    -p 3000:3000 \
    -e NODE_ENV=production \
    -e DATABASE_URL=${DATABASE_URL} \
    -v /var/log/myapp:/app/logs \
    ${IMAGE_TAG}

# 等待容器就绪
echo &quot;等待容器启动...&quot;
sleep 10

# 健康检查
if curl -f http://localhost:3000/health; then
    echo &quot;部署成功！&quot;
    # 清理旧镜像
    docker image prune -f
else
    echo &quot;部署失败，正在回滚...&quot;
    docker stop ${APP_NAME}
    exit 1
fi
</code></pre>
<h2>Docker Compose多服务编排</h2>
<h3>开发环境配置</h3>
<pre><code class="language-yaml"># docker-compose.dev.yml
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile.dev
    ports:
      - &quot;3000:3000&quot;
    volumes:
      # 代码热重载
      - .:/app
      - /app/node_modules
    environment:
      - NODE_ENV=development
      - DATABASE_URL=postgresql://user:password@db:5432/myapp_dev
      - REDIS_URL=redis://redis:6379
    depends_on:
      - db
      - redis
    networks:
      - app-network

  db:
    image: postgres:14-alpine
    ports:
      - &quot;5432:5432&quot;
    environment:
      - POSTGRES_DB=myapp_dev
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    ports:
      - &quot;6379:6379&quot;
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    networks:
      - app-network

  nginx:
    image: nginx:alpine
    ports:
      - &quot;80:80&quot;
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app
    networks:
      - app-network

volumes:
  postgres_data:
  redis_data:

networks:
  app-network:
    driver: bridge
</code></pre>
<h3>生产环境配置</h3>
<pre><code class="language-yaml"># docker-compose.prod.yml
version: '3.8'

services:
  app:
    image: myapp:${TAG:-latest}
    restart: unless-stopped
    ports:
      - &quot;3000:3000&quot;
    environment:
      - NODE_ENV=production
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    logging:
      driver: &quot;json-file&quot;
      options:
        max-size: &quot;10m&quot;
        max-file: &quot;3&quot;
    healthcheck:
      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:3000/health&quot;]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - app-network

  nginx:
    image: nginx:alpine
    restart: unless-stopped
    ports:
      - &quot;80:80&quot;
      - &quot;443:443&quot;
    volumes:
      - ./nginx.prod.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - app
    networks:
      - app-network

networks:
  app-network:
    external: true
</code></pre>
<h2>Kubernetes集群搭建</h2>
<h3>集群初始化配置</h3>
<pre><code class="language-yaml"># kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.1.100
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: k8s-api.example.com:6443
networking:
  serviceSubnet: 10.96.0.0/12
  podSubnet: 10.244.0.0/16
  dnsDomain: cluster.local
etcd:
  local:
    dataDir: /var/lib/etcd
apiServer:
  extraArgs:
    enable-admission-plugins: NodeRestriction,ResourceQuota
controllerManager:
  extraArgs:
    bind-address: 0.0.0.0
scheduler:
  extraArgs:
    bind-address: 0.0.0.0
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: systemd
</code></pre>
<h3>集群搭建脚本</h3>
<pre><code class="language-bash">#!/bin/bash
# setup-k8s-cluster.sh

set -e

# 系统准备
echo &quot;准备Kubernetes节点...&quot;

# 禁用swap
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# 加载内核模块
cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# 设置网络参数
cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system

# 安装containerd
echo &quot;安装containerd...&quot;
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

echo \
  &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null

sudo apt-get update
sudo apt-get install -y containerd.io

# 配置containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
sudo systemctl restart containerd
sudo systemctl enable containerd

# 安装kubeadm, kubelet, kubectl
echo &quot;安装Kubernetes组件...&quot;
curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg

echo &quot;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet=1.28.0-00 kubeadm=1.28.0-00 kubectl=1.28.0-00
sudo apt-mark hold kubelet kubeadm kubectl

# 初始化集群（仅在master节点执行）
if [[ &quot;$1&quot; == &quot;master&quot; ]]; then
    echo &quot;初始化Kubernetes集群...&quot;
    sudo kubeadm init --config=kubeadm-config.yaml

    # 配置kubectl
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

    # 安装网络插件（Calico）
    echo &quot;安装Calico网络插件...&quot;
    kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/tigera-operator.yaml
    kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/custom-resources.yaml

    echo &quot;集群初始化完成！&quot;
    echo &quot;使用以下命令将worker节点加入集群：&quot;
    sudo kubeadm token create --print-join-command
fi
</code></pre>
<h2>Kubernetes应用部署</h2>
<h3>应用Deployment配置</h3>
<pre><code class="language-yaml"># app-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: default
  labels:
    app: myapp
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
        version: v1
    spec:
      containers:
      - name: myapp
        image: myapp:1.2.3
        ports:
        - containerPort: 3000
          name: http
        env:
        - name: NODE_ENV
          value: &quot;production&quot;
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: myapp-secrets
              key: database-url
        - name: REDIS_URL
          valueFrom:
            configMapKeyRef:
              name: myapp-config
              key: redis-url
        resources:
          requests:
            memory: &quot;256Mi&quot;
            cpu: &quot;100m&quot;
          limits:
            memory: &quot;512Mi&quot;
            cpu: &quot;500m&quot;
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        securityContext:
          runAsNonRoot: true
          runAsUser: 1001
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: tmp
        emptyDir: {}
      - name: logs
        emptyDir: {}
      imagePullSecrets:
      - name: regcred
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
  labels:
    app: myapp
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 3000
    protocol: TCP
    name: http
  selector:
    app: myapp
</code></pre>
<h3>ConfigMap和Secret配置</h3>
<pre><code class="language-yaml"># app-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myapp-config
data:
  redis-url: &quot;redis://redis-service:6379&quot;
  log-level: &quot;info&quot;
  max-connections: &quot;100&quot;
  nginx.conf: |
    upstream backend {
        server myapp-service:80;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        location /health {
            access_log off;
            return 200 &quot;healthy\n&quot;;
            add_header Content-Type text/plain;
        }
    }
---
apiVersion: v1
kind: Secret
metadata:
  name: myapp-secrets
type: Opaque
stringData:
  database-url: &quot;postgresql://user:password@postgres-service:5432/myapp&quot;
  jwt-secret: &quot;your-super-secret-jwt-key&quot;
  api-key: &quot;your-external-api-key&quot;
</code></pre>
<h3>Ingress配置</h3>
<pre><code class="language-yaml"># app-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/rate-limit: &quot;100&quot;
    cert-manager.io/cluster-issuer: &quot;letsencrypt-prod&quot;
spec:
  tls:
  - hosts:
    - myapp.example.com
    secretName: myapp-tls
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp-service
            port:
              number: 80
</code></pre>
<h2>自动化部署流水线</h2>
<p>在整个CI/CD过程中，我们应用了spatiotemporal modeling来分析部署模式和时间窗口优化，使用lightweight engines进行快速镜像构建和测试，并创建了multi-modal data integration系统来整合代码质量、测试覆盖率和部署成功率等多维度数据。</p>
<h3>GitLab CI/CD配置</h3>
<pre><code class="language-yaml"># .gitlab-ci.yml
stages:
  - test
  - build
  - deploy

variables:
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: &quot;/certs&quot;
  REGISTRY: registry.gitlab.com
  IMAGE_NAME: $REGISTRY/$CI_PROJECT_PATH
  KUBECONFIG: /tmp/kubeconfig

before_script:
  - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY

# 代码质量检查
test:unit:
  stage: test
  image: node:16-alpine
  cache:
    paths:
      - node_modules/
  script:
    - npm ci
    - npm run test:coverage
    - npm run lint
    - npm run type-check
  coverage: '/All files[^|]*\|[^|]*\s+([\d\.]+)/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage/cobertura-coverage.xml
    paths:
      - coverage/
    expire_in: 1 week
  only:
    - merge_requests
    - main

# 安全扫描
test:security:
  stage: test
  image: owasp/zap2docker-stable
  script:
    - npm audit --audit-level=moderate
    - docker run --rm -v $(pwd):/src returntocorp/semgrep --config=auto /src
  allow_failure: true
  only:
    - merge_requests
    - main

# 构建Docker镜像
build:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  script:
    - export IMAGE_TAG=$IMAGE_NAME:$CI_COMMIT_SHA
    - export IMAGE_LATEST=$IMAGE_NAME:latest

    # 多阶段构建
    - docker build --pull 
      --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ')
      --build-arg VCS_REF=$CI_COMMIT_SHA
      --build-arg VERSION=$CI_COMMIT_TAG
      -t $IMAGE_TAG
      -t $IMAGE_LATEST
      .

    # 镜像安全扫描
    - docker run --rm -v /var/run/docker.sock:/var/run/docker.sock
      -v $(pwd):/tmp aquasec/trivy:latest image --exit-code 0 --severity HIGH,CRITICAL $IMAGE_TAG

    # 推送镜像
    - docker push $IMAGE_TAG
    - docker push $IMAGE_LATEST
  only:
    - main
    - tags

# 部署到开发环境
deploy:dev:
  stage: deploy
  image: bitnami/kubectl:latest
  environment:
    name: development
    url: https://dev.myapp.example.com
  script:
    - echo $KUBE_CONFIG_DEV | base64 -d &gt; $KUBECONFIG
    - export IMAGE_TAG=$IMAGE_NAME:$CI_COMMIT_SHA

    # 更新deployment镜像
    - kubectl set image deployment/myapp myapp=$IMAGE_TAG -n development

    # 等待部署完成
    - kubectl rollout status deployment/myapp -n development --timeout=300s

    # 验证部署
    - kubectl get pods -n development -l app=myapp
    - curl -f https://dev.myapp.example.com/health || exit 1
  only:
    - main
  when: manual

# 部署到生产环境
deploy:prod:
  stage: deploy
  image: bitnami/kubectl:latest
  environment:
    name: production
    url: https://myapp.example.com
  script:
    - echo $KUBE_CONFIG_PROD | base64 -d &gt; $KUBECONFIG
    - export IMAGE_TAG=$IMAGE_NAME:$CI_COMMIT_SHA

    # 蓝绿部署策略
    - |
      # 创建新版本deployment
      sed &quot;s|IMAGE_TAG|$IMAGE_TAG|g&quot; k8s/deployment-green.yaml | kubectl apply -f -

      # 等待新版本就绪
      kubectl rollout status deployment/myapp-green -n production --timeout=600s

      # 健康检查
      kubectl run --rm -i --restart=Never --image=curlimages/curl curl-test -- \
        curl -f http://myapp-green-service.production.svc.cluster.local/health

      # 切换流量到新版本
      kubectl patch service myapp-service -n production -p \
        '{&quot;spec&quot;:{&quot;selector&quot;:{&quot;version&quot;:&quot;green&quot;}}}'

      # 验证生产环境
      sleep 30
      curl -f https://myapp.example.com/health

      # 清理旧版本
      kubectl delete deployment myapp-blue -n production --ignore-not-found=true

      # 重命名新版本为blue（为下次部署准备）
      kubectl patch deployment myapp-green -n production -p \
        '{&quot;spec&quot;:{&quot;selector&quot;:{&quot;matchLabels&quot;:{&quot;version&quot;:&quot;blue&quot;}},&quot;template&quot;:{&quot;metadata&quot;:{&quot;labels&quot;:{&quot;version&quot;:&quot;blue&quot;}}}}}'
      kubectl patch deployment myapp-green -n production -p \
        '{&quot;metadata&quot;:{&quot;name&quot;:&quot;myapp-blue&quot;}}'
  only:
    - tags
  when: manual
</code></pre>
<h3>Helm Chart模板</h3>
<pre><code class="language-yaml"># charts/myapp/Chart.yaml
apiVersion: v2
name: myapp
description: My Application Helm Chart
type: application
version: 0.1.0
appVersion: &quot;1.0.0&quot;

dependencies:
  - name: postgresql
    version: 11.9.13
    repository: https://charts.bitnami.com/bitnami
    condition: postgresql.enabled
  - name: redis
    version: 17.3.7
    repository: https://charts.bitnami.com/bitnami
    condition: redis.enabled
</code></pre>
<pre><code class="language-yaml"># charts/myapp/values.yaml
replicaCount: 3

image:
  repository: myapp
  pullPolicy: IfNotPresent
  tag: &quot;latest&quot;

imagePullSecrets: []
nameOverride: &quot;&quot;
fullnameOverride: &quot;&quot;

serviceAccount:
  create: true
  annotations: {}
  name: &quot;&quot;

podAnnotations: {}

podSecurityContext:
  fsGroup: 1001

securityContext:
  runAsNonRoot: true
  runAsUser: 1001
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true

service:
  type: ClusterIP
  port: 80
  targetPort: 3000

ingress:
  enabled: true
  className: &quot;nginx&quot;
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: &quot;100&quot;
  hosts:
    - host: myapp.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: myapp-tls
      hosts:
        - myapp.example.com

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 100m
    memory: 256Mi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

# 数据库配置
postgresql:
  enabled: true
  auth:
    postgresPassword: &quot;secretpassword&quot;
    database: &quot;myapp&quot;
  primary:
    persistence:
      enabled: true
      size: 20Gi

# Redis配置
redis:
  enabled: true
  auth:
    enabled: false
  master:
    persistence:
      enabled: true
      size: 8Gi
</code></pre>
<pre><code class="language-yaml"># charts/myapp/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include &quot;myapp.fullname&quot; . }}
  labels:
    {{- include &quot;myapp.labels&quot; . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      {{- include &quot;myapp.selectorLabels&quot; . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include &quot;myapp.selectorLabels&quot; . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include &quot;myapp.serviceAccountName&quot; . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: &quot;{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}&quot;
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          env:
            - name: NODE_ENV
              value: &quot;production&quot;
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: {{ include &quot;myapp.fullname&quot; . }}-secrets
                  key: database-url
            - name: REDIS_URL
              value: &quot;redis://{{ include &quot;myapp.fullname&quot; . }}-redis-master:6379&quot;
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: logs
              mountPath: /app/logs
      volumes:
        - name: tmp
          emptyDir: {}
        - name: logs
          emptyDir: {}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
</code></pre>
<h2>监控和日志系统</h2>
<h3>Prometheus监控配置</h3>
<pre><code class="language-yaml"># monitoring/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    rule_files:
      - &quot;alert_rules.yml&quot;

    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093

    scrape_configs:
      # Kubernetes API Server
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https

      # 应用监控
      - job_name: 'myapp'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__

  alert_rules.yml: |
    groups:
    - name: myapp-alerts
      rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~&quot;5..&quot;}[5m]) &gt; 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: High error rate detected
          description: &quot;Error rate is {{ $value }} requests per second&quot;

      - alert: HighMemoryUsage
        expr: container_memory_usage_bytes / container_spec_memory_limit_bytes &gt; 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High memory usage
          description: &quot;Memory usage is above 80%&quot;

      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) &gt; 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Pod is crash looping
          description: &quot;Pod {{ $labels.pod }} is restarting frequently&quot;
</code></pre>
<h3>ELK日志收集</h3>
<pre><code class="language-yaml"># logging/filebeat-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
data:
  filebeat.yml: |
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*myapp*.log
      processors:
      - add_kubernetes_metadata:
          host: ${NODE_NAME}
          matchers:
          - logs_path:
              logs_path: &quot;/var/log/containers/&quot;

    output.elasticsearch:
      hosts: [&quot;elasticsearch:9200&quot;]
      template.settings:
        index.number_of_shards: 1
        index.codec: best_compression

    setup.kibana:
      host: &quot;kibana:5601&quot;

    logging.level: info
    logging.to_files: true
    logging.files:
      path: /var/log/filebeat
      name: filebeat
      keepfiles: 7
      permissions: 0644
</code></pre>
<h2>性能优化和最佳实践</h2>
<h3>资源限制和HPA配置</h3>
<pre><code class="language-yaml"># hpa-config.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max
</code></pre>
<h3>网络策略安全配置</h3>
<pre><code class="language-yaml"># network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: myapp-network-policy
spec:
  podSelector:
    matchLabels:
      app: myapp
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # 允许来自nginx-ingress的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 3000
  # 允许来自监控系统的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 3000
  egress:
  # 允许访问数据库
  - to:
    - podSelector:
        matchLabels:
          app: postgresql
    ports:
    - protocol: TCP
      port: 5432
  # 允许访问Redis
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  # 允许DNS查询
  - to: []
    ports:
    - protocol: UDP
      port: 53
</code></pre>
<h2>效果总结和经验教训</h2>
<h3>性能提升数据</h3>
<pre><code class="language-javascript">const improvementMetrics = {
  deploymentSpeed: {
    before: '120分钟（手动部署）',
    after: '5分钟（自动化部署）',
    improvement: '96%提升'
  },

  scalability: {
    before: '30分钟手动扩容',
    after: '30秒自动扩容',
    improvement: '99%提升'
  },

  resourceUtilization: {
    before: '15% CPU平均使用率',
    after: '60% CPU平均使用率',
    improvement: '4倍资源利用率提升'
  },

  reliability: {
    uptime: '从99.5%提升到99.95%',
    mttr: '从4小时降低到15分钟',
    rollbackTime: '从60分钟降低到2分钟'
  },

  costOptimization: {
    infrastructure: '40%基础设施成本节省',
    operations: '60%运维工作量减少',
    development: '50%开发部署时间节省'
  }
};
</code></pre>
<h3>关键经验教训</h3>
<pre><code class="language-markdown">## 成功因素

### 1. 渐进式迁移策略
- 从单个应用开始容器化
- 逐步引入Kubernetes特性
- 保持旧系统并行运行直到稳定

### 2. 完善的监控体系
- 从第一天就建立监控和告警
- 日志聚合和分析系统
- 性能指标和业务指标结合

### 3. 自动化优先
- CI/CD流水线自动化
- 基础设施即代码
- 自动化测试和安全扫描

### 4. 团队技能提升
- 定期技术培训和分享
- 建立最佳实践文档
- 鼓励实验和学习

## 遇到的挑战

### 1. 技术复杂度
- Kubernetes学习曲线陡峭
- 网络和存储配置复杂
- 调试分布式系统困难

### 2. 文化转变
- 从宠物服务器到牲畜服务器的思维转变
- DevOps文化建设
- 团队协作模式调整

### 3. 安全性考虑
- 容器镜像安全扫描
- 网络策略配置
- 密钥管理和轮换
</code></pre>
<h2>未来发展方向</h2>
<pre><code class="language-javascript">const futureRoadmap = {
  shortTerm: [
    '引入Service Mesh（Istio）',
    '实现多集群管理',
    '优化成本管理和资源调度',
    '完善灾难恢复方案'
  ],

  mediumTerm: [
    'GitOps工作流实践',
    '云原生安全最佳实践',
    '无服务器计算集成',
    '边缘计算部署'
  ],

  longTerm: [
    'AI驱动的智能运维',
    '混合云和多云策略',
    '量子计算就绪架构',
    '绿色计算和可持续发展'
  ]
};
</code></pre>
<h2>结语</h2>
<p>从传统虚拟机部署到云原生Kubernetes架构的转型，不仅仅是技术栈的升级，更是整个开发运维理念的革新。这个过程中我们学到：</p>
<ol>
<li><strong>技术选型要考虑团队实际情况</strong>：不要为了技术而技术，要解决实际问题</li>
<li><strong>循序渐进比一步到位更安全</strong>：大型系统改造需要充分的规划和测试</li>
<li><strong>监控和自动化是成功的关键</strong>：没有可观测性就没有可维护性</li>
<li><strong>团队能力建设同样重要</strong>：技术转型的成功依赖于人的成长</li>
<li><strong>安全性要从设计阶段就考虑</strong>：后期补救成本远高于前期投入</li>
</ol>
<p>容器化和Kubernetes为我们带来了前所未有的灵活性和效率，但也要求我们以更系统化、工程化的方式思考和实践。这条路充满挑战，但收益是巨大的——更快的迭代速度、更高的系统可靠性、更低的运维成本。</p>
<p>对于正在考虑类似转型的团队，我的建议是：制定清晰的迁移计划，投资于团队培训，建立完善的测试和监控体系，然后勇敢地迈出第一步。云原生的世界正在等着你！</p>
                </div>
            </article>
        </div>
    </main>
    
    <footer>
        <p>&copy; 2025 我的博客. All rights reserved.</p>
    </footer>
</body>
</html>