<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kubernetesç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µä¸è¸©å‘æ€»ç»“ - æˆ‘çš„åšå®¢</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1 class="slogan">è®°å½•æ€è€ƒï¼Œåˆ†äº«ç”Ÿæ´»</h1>
    </header>
    
    <main>
        <div class="container">
            <a href="../index.html" class="back-link">â† è¿”å›é¦–é¡µ</a>
            
            <article class="article-page">
                <div class="article-header">
                    <h1>Kubernetesç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µä¸è¸©å‘æ€»ç»“</h1>
                    <p class="article-date">2025å¹´07æœˆ03æ—¥</p>
                </div>
                
                <div class="article-content">
                    <p><strong>Date: May 24, 2024</strong></p>
<p>æœ€è¿‘åœ¨å…¬å¸è´Ÿè´£å°†å‡ ä¸ªæ ¸å¿ƒæœåŠ¡è¿ç§»åˆ°Kuberneteså¹³å°ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ç§¯ç´¯äº†ä¸å°‘ç”Ÿäº§ç¯å¢ƒçš„æœ€ä½³å®è·µç»éªŒï¼Œä¹Ÿè¸©äº†ä¸å°‘å‘ã€‚ä»Šå¤©æ•´ç†ä¸€ä¸‹è¿™äº›ç»éªŒï¼Œå¸Œæœ›èƒ½å¸®åŠ©åˆ°å…¶ä»–æ­£åœ¨æˆ–å‡†å¤‡ä½¿ç”¨K8sçš„åŒå­¦ã€‚</p>
<h2>é›†ç¾¤æ¶æ„è®¾è®¡</h2>
<h3>é«˜å¯ç”¨é›†ç¾¤æ­å»º</h3>
<pre><code class="language-yaml"># é«˜å¯ç”¨MasterèŠ‚ç‚¹é…ç½®
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.27.0
controlPlaneEndpoint: k8s-api.company.com:6443
networking:
  serviceSubnet: 10.96.0.0/12
  podSubnet: 10.244.0.0/16
etcd:
  external:
    endpoints:
    - https://etcd1.company.com:2379
    - https://etcd2.company.com:2379
    - https://etcd3.company.com:2379
    caFile: /etc/ssl/etcd/ca.pem
    certFile: /etc/ssl/etcd/client.pem
    keyFile: /etc/ssl/etcd/client-key.pem
apiServer:
  extraArgs:
    audit-log-maxage: &quot;30&quot;
    audit-log-maxbackup: &quot;10&quot;
    audit-log-maxsize: &quot;100&quot;
    audit-log-path: /var/log/audit.log
    enable-admission-plugins: NodeRestriction,ResourceQuota,PodSecurityPolicy
controllerManager:
  extraArgs:
    node-monitor-grace-period: 40s
    node-monitor-period: 5s
    pod-eviction-timeout: 5m0s
scheduler:
  extraArgs:
    kube-api-qps: &quot;100&quot;
    kube-api-burst: &quot;100&quot;
</code></pre>
<h3>èŠ‚ç‚¹è§„åˆ’ä¸æ ‡ç­¾ç®¡ç†</h3>
<pre><code class="language-bash"># èŠ‚ç‚¹æ ‡ç­¾ç­–ç•¥
kubectl label nodes k8s-worker-1 node-role.kubernetes.io/worker=
kubectl label nodes k8s-worker-1 node-type=compute
kubectl label nodes k8s-worker-1 zone=zone-a
kubectl label nodes k8s-worker-1 instance-type=c5.2xlarge

# æ±¡ç‚¹ç­–ç•¥ç”¨äºä¸“ç”¨èŠ‚ç‚¹
kubectl taint nodes k8s-gpu-1 gpu=true:NoSchedule
kubectl taint nodes k8s-db-1 database=true:NoSchedule

# ä¸ºä¸åŒç±»å‹çš„å·¥ä½œè´Ÿè½½åˆ›å»ºä¸“ç”¨èŠ‚ç‚¹æ± 
# è®¡ç®—å¯†é›†å‹èŠ‚ç‚¹
kubectl label nodes k8s-compute-* workload-type=compute-intensive

# å†…å­˜å¯†é›†å‹èŠ‚ç‚¹  
kubectl label nodes k8s-memory-* workload-type=memory-intensive

# å­˜å‚¨å¯†é›†å‹èŠ‚ç‚¹
kubectl label nodes k8s-storage-* workload-type=storage-intensive
</code></pre>
<h2>åº”ç”¨éƒ¨ç½²æœ€ä½³å®è·µ</h2>
<h3>ç”Ÿäº§çº§Deploymenté…ç½®</h3>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: production
  labels:
    app: web-app
    version: v1.2.3
    environment: production
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # ç¡®ä¿é›¶åœæœºæ—¶é—´
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
        version: v1.2.3
      annotations:
        prometheus.io/scrape: &quot;true&quot;
        prometheus.io/port: &quot;8080&quot;
        prometheus.io/path: &quot;/metrics&quot;
    spec:
      # å®‰å…¨ä¸Šä¸‹æ–‡
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000

      # èŠ‚ç‚¹é€‰æ‹©
      nodeSelector:
        workload-type: compute-intensive

      # å®¹å¿åº¦é…ç½®
      tolerations:
      - key: &quot;workload-type&quot;
        operator: &quot;Equal&quot;
        value: &quot;compute-intensive&quot;
        effect: &quot;NoSchedule&quot;

      # Podåäº²å’Œæ€§ï¼Œç¡®ä¿Podåˆ†å¸ƒåœ¨ä¸åŒèŠ‚ç‚¹
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web-app
              topologyKey: kubernetes.io/hostname

        # èŠ‚ç‚¹äº²å’Œæ€§
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: zone
                operator: In
                values:
                - zone-a
                - zone-b

      containers:
      - name: web-app
        image: myregistry/web-app:v1.2.3
        imagePullPolicy: Always

        # ç«¯å£é…ç½®
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP

        # ç¯å¢ƒå˜é‡
        env:
        - name: NODE_ENV
          value: &quot;production&quot;
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: url
        - name: REDIS_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: redis-url

        # èµ„æºé™åˆ¶
        resources:
          requests:
            memory: &quot;256Mi&quot;
            cpu: &quot;250m&quot;
          limits:
            memory: &quot;512Mi&quot;
            cpu: &quot;500m&quot;

        # å¥åº·æ£€æŸ¥
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2

        # å¯åŠ¨æ¢é’ˆï¼ˆç”¨äºæ…¢å¯åŠ¨åº”ç”¨ï¼‰
        startupProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30

        # å®‰å…¨ä¸Šä¸‹æ–‡
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL

        # æŒ‚è½½ç‚¹
        volumeMounts:
        - name: app-logs
          mountPath: /var/log/app
        - name: temp
          mountPath: /tmp
        - name: app-config
          mountPath: /app/config
          readOnly: true

      # å­˜å‚¨å·
      volumes:
      - name: app-logs
        emptyDir: {}
      - name: temp
        emptyDir: {}
      - name: app-config
        configMap:
          name: app-config
          defaultMode: 0644

      # é•œåƒæ‹‰å–å¯†é’¥
      imagePullSecrets:
      - name: registry-credentials

      # ç»ˆæ­¢å®½é™æœŸ
      terminationGracePeriodSeconds: 30
</code></pre>
<h3>ConfigMapå’ŒSecretç®¡ç†</h3>
<pre><code class="language-yaml"># ConfigMapé…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
data:
  redis-url: &quot;redis://redis-service:6379&quot;
  log-level: &quot;info&quot;
  max-connections: &quot;100&quot;
  app.properties: |
    server.port=8080
    management.endpoints.web.exposure.include=health,metrics,info
    logging.level.com.company=INFO
    spring.datasource.hikari.maximum-pool-size=20

---
# Secreté…ç½®
apiVersion: v1
kind: Secret
metadata:
  name: db-credentials
  namespace: production
type: Opaque
data:
  url: cG9zdGdyZXNxbDovL3VzZXI6cGFzc3dvcmRAbG9jYWxob3N0OjU0MzIvZGI=  # base64ç¼–ç 
  username: dXNlcg==
  password: cGFzc3dvcmQ=

---
# Docker Registry Secret
apiVersion: v1
kind: Secret
metadata:
  name: registry-credentials
  namespace: production
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: eyJhdXRocyI6eyJteXJlZ2lzdHJ5LmNvbSI6eyJ1c2VybmFtZSI6InVzZXIiLCJwYXNzd29yZCI6InBhc3MiLCJhdXRoIjoiZFhObGNqcHdZWE56In19fQ==
</code></pre>
<h2>æœåŠ¡å‘ç°ä¸è´Ÿè½½å‡è¡¡</h2>
<h3>Serviceé…ç½®æœ€ä½³å®è·µ</h3>
<pre><code class="language-yaml"># ClusterIP Serviceç”¨äºé›†ç¾¤å†…é€šä¿¡
apiVersion: v1
kind: Service
metadata:
  name: web-app-service
  namespace: production
  labels:
    app: web-app
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: web-app
  sessionAffinity: None  # æˆ–è€…ClientIPç”¨äºä¼šè¯ç²˜æ€§

---
# LoadBalancer Serviceç”¨äºå¤–éƒ¨è®¿é—®
apiVersion: v1
kind: Service
metadata:
  name: web-app-lb
  namespace: production
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-west-2:123456789:certificate/12345
    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: &quot;https&quot;
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
spec:
  type: LoadBalancer
  ports:
  - port: 443
    targetPort: 8080
    protocol: TCP
    name: https
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: web-app
</code></pre>
<h3>Ingressé…ç½®</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-app-ingress
  namespace: production
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;
    nginx.ingress.kubernetes.io/rate-limit: &quot;100&quot;
    nginx.ingress.kubernetes.io/rate-limit-window: &quot;1m&quot;
    cert-manager.io/cluster-issuer: letsencrypt-prod
    # ä¼šè¯äº²å’Œæ€§
    nginx.ingress.kubernetes.io/affinity: cookie
    nginx.ingress.kubernetes.io/session-cookie-name: INGRESSCOOKIE
    nginx.ingress.kubernetes.io/session-cookie-expires: &quot;86400&quot;
    nginx.ingress.kubernetes.io/session-cookie-max-age: &quot;86400&quot;
    nginx.ingress.kubernetes.io/session-cookie-path: /
spec:
  tls:
  - hosts:
    - api.company.com
    secretName: api-tls-secret
  rules:
  - host: api.company.com
    http:
      paths:
      - path: /api/v1
        pathType: Prefix
        backend:
          service:
            name: web-app-service
            port:
              number: 80
      - path: /health
        pathType: Exact
        backend:
          service:
            name: web-app-service
            port:
              number: 80
</code></pre>
<h2>å­˜å‚¨ç®¡ç†</h2>
<h3>PersistentVolumeé…ç½®</h3>
<pre><code class="language-yaml"># StorageClasså®šä¹‰
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  iops: &quot;3000&quot;
  throughput: &quot;125&quot;
  encrypted: &quot;true&quot;
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete

---
# StatefulSet with persistent storage
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
  namespace: production
spec:
  serviceName: database-headless
  replicas: 3
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      containers:
      - name: postgres
        image: postgres:14
        env:
        - name: POSTGRES_DB
          value: myapp
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: username
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        ports:
        - containerPort: 5432
          name: postgres
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: 1Gi
            cpu: 500m
          limits:
            memory: 2Gi
            cpu: 1000m
        livenessProbe:
          exec:
            command:
            - /usr/bin/pg_isready
            - -h
            - localhost
            - -U
            - $(POSTGRES_USER)
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - /usr/bin/pg_isready
            - -h
            - localhost
            - -U
            - $(POSTGRES_USER)
          initialDelaySeconds: 5
          periodSeconds: 5
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
</code></pre>
<h2>èµ„æºç®¡ç†ä¸è‡ªåŠ¨æ‰©ç¼©å®¹</h2>
<h3>HorizontalPodAutoscaleré…ç½®</h3>
<pre><code class="language-yaml">apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: 100
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max
</code></pre>
<h3>VerticalPodAutoscaleré…ç½®</h3>
<pre><code class="language-yaml">apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: &quot;Auto&quot;  # æˆ–è€…&quot;Off&quot;, &quot;Initial&quot;, &quot;Recreation&quot;
  resourcePolicy:
    containerPolicies:
    - containerName: web-app
      maxAllowed:
        cpu: 2000m
        memory: 4Gi
      minAllowed:
        cpu: 100m
        memory: 128Mi
      controlledResources: [&quot;cpu&quot;, &quot;memory&quot;]
</code></pre>
<h3>é›†ç¾¤è‡ªåŠ¨æ‰©ç¼©å®¹</h3>
<pre><code class="language-yaml"># Cluster Autoscaleré…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.27.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/production
        - --balance-similar-node-groups
        - --scale-down-enabled=true
        - --scale-down-delay-after-add=10m
        - --scale-down-unneeded-time=10m
        - --scale-down-utilization-threshold=0.5
        - --skip-nodes-with-system-pods=false
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 100m
            memory: 300Mi
</code></pre>
<h2>ç›‘æ§ä¸æ—¥å¿—</h2>
<h3>Prometheusç›‘æ§é…ç½®</h3>
<pre><code class="language-yaml"># ServiceMonitor for application metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: web-app-metrics
  namespace: production
  labels:
    app: web-app
spec:
  selector:
    matchLabels:
      app: web-app
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s
    honorLabels: true

---
# PrometheusRule for alerting
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: web-app-alerts
  namespace: production
spec:
  groups:
  - name: web-app.rules
    rules:
    - alert: HighCPUUsage
      expr: rate(container_cpu_usage_seconds_total{pod=~&quot;web-app-.*&quot;}[5m]) * 100 &gt; 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: &quot;High CPU usage detected&quot;
        description: &quot;Pod {{ $labels.pod }} CPU usage is above 80%&quot;

    - alert: HighMemoryUsage
      expr: container_memory_usage_bytes{pod=~&quot;web-app-.*&quot;} / container_spec_memory_limit_bytes * 100 &gt; 90
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: &quot;High memory usage detected&quot;
        description: &quot;Pod {{ $labels.pod }} memory usage is above 90%&quot;

    - alert: PodCrashLooping
      expr: increase(kube_pod_container_status_restarts_total{pod=~&quot;web-app-.*&quot;}[1h]) &gt; 5
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: &quot;Pod is crash looping&quot;
        description: &quot;Pod {{ $labels.pod }} has restarted more than 5 times in the last hour&quot;
</code></pre>
<h3>æ—¥å¿—æ”¶é›†é…ç½®</h3>
<pre><code class="language-yaml"># Fluent Bit DaemonSeté…ç½®
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: logging
spec:
  selector:
    matchLabels:
      name: fluent-bit
  template:
    metadata:
      labels:
        name: fluent-bit
    spec:
      serviceAccountName: fluent-bit
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:2.1.8
        ports:
        - containerPort: 2020
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: &quot;elasticsearch-service&quot;
        - name: FLUENT_ELASTICSEARCH_PORT
          value: &quot;9200&quot;
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluent-bit-config
          mountPath: /fluent-bit/etc/
        resources:
          limits:
            memory: 200Mi
            cpu: 100m
          requests:
            memory: 100Mi
            cpu: 50m
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluent-bit-config
        configMap:
          name: fluent-bit-config

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: logging
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
        HTTP_Server   On
        HTTP_Listen   0.0.0.0
        HTTP_Port     2020

    [INPUT]
        Name              tail
        Path              /var/log/containers/*.log
        Parser            docker
        Tag               kube.*
        Refresh_Interval  5
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On

    [FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On
        Keep_Log            Off
        K8S-Logging.Parser  On
        K8S-Logging.Exclude Off

    [OUTPUT]
        Name  es
        Match *
        Host  ${FLUENT_ELASTICSEARCH_HOST}
        Port  ${FLUENT_ELASTICSEARCH_PORT}
        Index fluent-bit
        Type  _doc
</code></pre>
<h2>å®‰å…¨å®è·µ</h2>
<h3>RBACé…ç½®</h3>
<pre><code class="language-yaml"># ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: web-app-sa
  namespace: production

---
# Roleå®šä¹‰
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: web-app-role
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;configmaps&quot;, &quot;secrets&quot;]
  verbs: [&quot;get&quot;, &quot;list&quot;]
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods&quot;]
  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]

---
# RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: web-app-rolebinding
  namespace: production
subjects:
- kind: ServiceAccount
  name: web-app-sa
  namespace: production
roleRef:
  kind: Role
  name: web-app-role
  apiGroup: rbac.authorization.k8s.io
</code></pre>
<h3>NetworkPolicyé…ç½®</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: web-app-netpol
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: web-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    - podSelector:
        matchLabels:
          app: nginx-ingress
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: database
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  - to: []  # å…è®¸DNSæŸ¥è¯¢
    ports:
    - protocol: UDP
      port: 53
</code></pre>
<h3>PodSecurityPolicyé…ç½®</h3>
<pre><code class="language-yaml">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: true
</code></pre>
<h2>è¸©å‘æ€»ç»“</h2>
<h3>1. èµ„æºé™åˆ¶è®¾ç½®ä¸å½“</h3>
<pre><code class="language-yaml"># é”™è¯¯ç¤ºä¾‹ï¼šèµ„æºé™åˆ¶è¿‡ä½
resources:
  limits:
    memory: &quot;128Mi&quot;  # å¤ªä½ï¼Œå®¹æ˜“OOMKilled
    cpu: &quot;100m&quot;      # å¤ªä½ï¼Œæ€§èƒ½ä¸è¶³
  requests:
    memory: &quot;64Mi&quot;   # å¤ªä½
    cpu: &quot;50m&quot;       # å¤ªä½

# æ­£ç¡®ç¤ºä¾‹ï¼šåˆç†çš„èµ„æºé™åˆ¶
resources:
  limits:
    memory: &quot;512Mi&quot;  # ç»™è¶³å¤Ÿçš„å†…å­˜ç©ºé—´
    cpu: &quot;500m&quot;      # é€‚å½“çš„CPUé™åˆ¶
  requests:
    memory: &quot;256Mi&quot;  # è¯·æ±‚å€¼æ˜¯é™åˆ¶å€¼çš„ä¸€åŠ
    cpu: &quot;250m&quot;      # ä¿è¯åŸºæœ¬æ€§èƒ½éœ€æ±‚
</code></pre>
<h3>2. å¥åº·æ£€æŸ¥é…ç½®é—®é¢˜</h3>
<pre><code class="language-yaml"># é”™è¯¯é…ç½®ï¼šæ¢é’ˆé—´éš”å¤ªçŸ­
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 5   # å¤ªçŸ­ï¼Œåº”ç”¨å¯èƒ½è¿˜æ²¡å¯åŠ¨å®Œæˆ
  periodSeconds: 5         # å¤ªé¢‘ç¹
  timeoutSeconds: 1        # å¤ªçŸ­ï¼Œç½‘ç»œæŠ–åŠ¨å°±å¤±è´¥
  failureThreshold: 1      # å¤ªä¸¥æ ¼ï¼Œä¸€æ¬¡å¤±è´¥å°±é‡å¯

# æ­£ç¡®é…ç½®
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30  # ç»™åº”ç”¨è¶³å¤Ÿçš„å¯åŠ¨æ—¶é—´
  periodSeconds: 10        # åˆç†çš„æ£€æŸ¥é—´éš”
  timeoutSeconds: 5        # è¶³å¤Ÿçš„è¶…æ—¶æ—¶é—´
  failureThreshold: 3      # å…è®¸å¶å‘å¤±è´¥
</code></pre>
<h3>3. HPAé…ç½®é™·é˜±</h3>
<pre><code class="language-bash"># å¸¸è§é—®é¢˜ï¼šmetrics-serveræ²¡æœ‰æ­£ç¡®å®‰è£…æˆ–é…ç½®
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# æ£€æŸ¥metrics-serverçŠ¶æ€
kubectl get deployment metrics-server -n kube-system

# æŸ¥çœ‹èŠ‚ç‚¹èµ„æºä½¿ç”¨æƒ…å†µ
kubectl top nodes

# æŸ¥çœ‹Podèµ„æºä½¿ç”¨æƒ…å†µ
kubectl top pods -n production
</code></pre>
<h3>4. å­˜å‚¨ç›¸å…³é—®é¢˜</h3>
<pre><code class="language-bash"># å¸¸è§é—®é¢˜ï¼šPVå›æ”¶ç­–ç•¥ä¸å½“å¯¼è‡´æ•°æ®ä¸¢å¤±
# æ£€æŸ¥StorageClassçš„å›æ”¶ç­–ç•¥
kubectl get storageclass -o yaml

# ä¿®æ”¹PVçš„å›æ”¶ç­–ç•¥
kubectl patch pv pv-name -p '{&quot;spec&quot;:{&quot;persistentVolumeReclaimPolicy&quot;:&quot;Retain&quot;}}'

# æŸ¥çœ‹PVCçŠ¶æ€
kubectl get pvc -A

# æ’æŸ¥å­˜å‚¨é—®é¢˜
kubectl describe pvc pvc-name -n namespace
</code></pre>
<h3>5. ç½‘ç»œé—®é¢˜æ’æŸ¥</h3>
<pre><code class="language-bash"># æ£€æŸ¥Podç½‘ç»œè¿é€šæ€§
kubectl exec -it pod-name -- ping target-pod-ip

# æ£€æŸ¥æœåŠ¡å‘ç°
kubectl exec -it pod-name -- nslookup service-name

# æ£€æŸ¥NetworkPolicyæ˜¯å¦é˜»æ­¢äº†é€šä¿¡
kubectl get networkpolicy -n namespace

# æŸ¥çœ‹IngressçŠ¶æ€
kubectl get ingress -A
kubectl describe ingress ingress-name -n namespace
</code></pre>
<h2>ç”Ÿäº§ç¯å¢ƒchecklist</h2>
<h3>éƒ¨ç½²å‰æ£€æŸ¥</h3>
<pre><code class="language-bash">#!/bin/bash
# ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ£€æŸ¥è„šæœ¬

echo &quot;ğŸ” å¼€å§‹ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‰æ£€æŸ¥...&quot;

# 1. æ£€æŸ¥èµ„æºé…é¢
echo &quot;1. æ£€æŸ¥èµ„æºé…é¢...&quot;
kubectl describe quota -n production

# 2. æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
echo &quot;2. æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€...&quot;
kubectl get nodes -o wide

# 3. æ£€æŸ¥é•œåƒæ˜¯å¦å­˜åœ¨
echo &quot;3. æ£€æŸ¥é•œåƒ...&quot;
IMAGE=$(grep &quot;image:&quot; deployment.yaml | awk '{print $2}')
docker pull $IMAGE &amp;&amp; echo &quot;âœ… é•œåƒæ£€æŸ¥é€šè¿‡&quot; || echo &quot;âŒ é•œåƒæ‹‰å–å¤±è´¥&quot;

# 4. éªŒè¯é…ç½®æ–‡ä»¶
echo &quot;4. éªŒè¯é…ç½®æ–‡ä»¶...&quot;
kubectl apply --dry-run=client -f . &amp;&amp; echo &quot;âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡&quot; || echo &quot;âŒ é…ç½®æ–‡ä»¶æœ‰è¯¯&quot;

# 5. æ£€æŸ¥ä¾èµ–æœåŠ¡
echo &quot;5. æ£€æŸ¥ä¾èµ–æœåŠ¡...&quot;
kubectl get svc database-service redis-service -n production

# 6. æ£€æŸ¥å­˜å‚¨
echo &quot;6. æ£€æŸ¥å­˜å‚¨...&quot;
kubectl get pvc -n production

# 7. æ£€æŸ¥å®‰å…¨ç­–ç•¥
echo &quot;7. æ£€æŸ¥å®‰å…¨ç­–ç•¥...&quot;
kubectl auth can-i create pods --as=system:serviceaccount:production:web-app-sa -n production

echo &quot;âœ… éƒ¨ç½²å‰æ£€æŸ¥å®Œæˆ&quot;
</code></pre>
<h3>éƒ¨ç½²åéªŒè¯</h3>
<pre><code class="language-bash">#!/bin/bash
# éƒ¨ç½²åéªŒè¯è„šæœ¬

NAMESPACE=&quot;production&quot;
APP_NAME=&quot;web-app&quot;

echo &quot;ğŸ” å¼€å§‹éƒ¨ç½²åéªŒè¯...&quot;

# 1. æ£€æŸ¥PodçŠ¶æ€
echo &quot;1. æ£€æŸ¥PodçŠ¶æ€...&quot;
kubectl get pods -n $NAMESPACE -l app=$APP_NAME

# 2. æ£€æŸ¥æœåŠ¡ç«¯ç‚¹
echo &quot;2. æ£€æŸ¥æœåŠ¡ç«¯ç‚¹...&quot;
kubectl get endpoints -n $NAMESPACE

# 3. æ£€æŸ¥å¥åº·æ£€æŸ¥
echo &quot;3. æ£€æŸ¥å¥åº·æ£€æŸ¥...&quot;
kubectl describe pod -n $NAMESPACE -l app=$APP_NAME | grep -A 5 &quot;Liveness\|Readiness&quot;

# 4. æ£€æŸ¥æ—¥å¿—
echo &quot;4. æ£€æŸ¥åº”ç”¨æ—¥å¿—...&quot;
kubectl logs -n $NAMESPACE -l app=$APP_NAME --tail=50

# 5. æ£€æŸ¥æŒ‡æ ‡
echo &quot;5. æ£€æŸ¥æŒ‡æ ‡...&quot;
kubectl top pods -n $NAMESPACE -l app=$APP_NAME

# 6. è¿›è¡Œå¥åº·æ£€æŸ¥è°ƒç”¨
echo &quot;6. è¿›è¡Œå¥åº·æ£€æŸ¥...&quot;
SERVICE_IP=$(kubectl get svc $APP_NAME-service -n $NAMESPACE -o jsonpath='{.spec.clusterIP}')
kubectl run test-pod --rm -i --tty --image=curlimages/curl -- curl http://$SERVICE_IP/health

echo &quot;âœ… éƒ¨ç½²åéªŒè¯å®Œæˆ&quot;
</code></pre>
<h2>æ€»ç»“</h2>
<p>Kubernetesç”Ÿäº§ç¯å¢ƒçš„è¿ç»´ç¡®å®å¤æ‚ï¼Œä½†é€šè¿‡åˆç†çš„é…ç½®å’Œæœ€ä½³å®è·µï¼Œå¯ä»¥æ„å»ºå‡ºç¨³å®šã€é«˜æ•ˆçš„å®¹å™¨åŒ–å¹³å°ã€‚å…³é”®è¦ç‚¹ï¼š</p>
<ol>
<li><strong>å®‰å…¨ç¬¬ä¸€</strong>ï¼šRBACã€NetworkPolicyã€SecurityContextä¸€ä¸ªéƒ½ä¸èƒ½å°‘</li>
<li><strong>ç›‘æ§å®Œå–„</strong>ï¼šæŒ‡æ ‡ã€æ—¥å¿—ã€å‘Šè­¦ä½“ç³»è¦å®Œæ•´</li>
<li><strong>èµ„æºåˆç†</strong>ï¼šCPUã€å†…å­˜ã€å­˜å‚¨é…ç½®è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´</li>
<li><strong>é«˜å¯ç”¨è®¾è®¡</strong>ï¼šå¤šå‰¯æœ¬ã€åäº²å’Œæ€§ã€å¥åº·æ£€æŸ¥ä¸å¯ç¼ºå°‘</li>
<li><strong>è‡ªåŠ¨åŒ–è¿ç»´</strong>ï¼šHPAã€VPAã€é›†ç¾¤è‡ªåŠ¨æ‰©ç¼©å®¹æé«˜è¿ç»´æ•ˆç‡</li>
</ol>
<p>åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨Kubernetesæ˜¯ä¸€ä¸ªæŒç»­å­¦ä¹ å’Œä¼˜åŒ–çš„è¿‡ç¨‹ï¼Œå¸Œæœ›è¿™äº›ç»éªŒèƒ½å¸®åŠ©å¤§å®¶å°‘è¸©ä¸€äº›å‘ï¼Œæ›´å¿«åœ°æ„å»ºç¨³å®šçš„K8sé›†ç¾¤ã€‚</p>
                </div>
            </article>
        </div>
    </main>
    
    <footer>
        <p>&copy; 2025 æˆ‘çš„åšå®¢. All rights reserved.</p>
    </footer>
</body>
</html>