<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Learning Journey: From Monolith to Microservices - 我的博客</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1 class="slogan">记录思考，分享生活</h1>
    </header>
    
    <main>
        <div class="container">
            <a href="../index.html" class="back-link">← 返回首页</a>
            
            <article class="article-page">
                <div class="article-header">
                    <h1>Learning Journey: From Monolith to Microservices</h1>
                    <p class="article-date">2024年04月05日</p>
                </div>
                
                <div class="article-content">
                    <hr />
<p>title: "Learning Journey: From Monolith to Microservices"<br />
date: "2024-04-05"<br />
tags: ["architecture", "microservices", "learning", "personal-growth"]</p>
<hr />
<h1>Learning Journey: From Monolith to Microservices</h1>
<p>Two years ago, I embarked on what would become one of the most challenging and rewarding learning experiences of my career: transitioning our team's monolithic application to a microservices architecture. This journey wasn't just about technology—it was about changing mindsets, learning new patterns, and discovering the true meaning of distributed systems.</p>
<h2>The Starting Point</h2>
<p>Our monolithic application had served us well for three years. It was a Ruby on Rails application handling everything from user authentication to payment processing, inventory management, and reporting. The codebase had grown to over 200,000 lines of code, and we had a team of 12 developers working on it.</p>
<p>The pain points were becoming impossible to ignore:<br />
- Deployment took 45 minutes and required careful coordination<br />
- A bug in one module could bring down the entire system<br />
- Scaling meant scaling everything, even if only one component needed more resources<br />
- New developers took weeks to understand the codebase<br />
- Technology choices were locked in—we couldn't experiment with new languages or frameworks</p>
<h2>The Decision to Change</h2>
<p>The tipping point came during a particularly bad week where we had three production incidents, each caused by seemingly unrelated changes. That's when our CTO called a meeting and said, "We need to talk about our architecture."</p>
<p>I remember the excitement and fear I felt when I volunteered to lead the research into microservices. Excitement because I'd been reading about companies like Netflix and Uber successfully using microservices. Fear because I knew this would be a massive undertaking that could either propel our team forward or set us back months.</p>
<h2>Phase 1: Learning and Planning</h2>
<h3>Deep Dive into Microservices Literature</h3>
<p>I started by reading everything I could get my hands on:<br />
- "Building Microservices" by Sam Newman<br />
- "Microservices Patterns" by Chris Richardson<br />
- Countless blog posts from engineering teams at major tech companies<br />
- Academic papers on distributed systems</p>
<p>But theoretical knowledge only goes so far. I needed to understand how these patterns would apply to our specific context.</p>
<h3>Building a Proof of Concept</h3>
<p>I spent evenings and weekends building a small proof of concept. I extracted our user authentication service into a separate application:</p>
<pre><code class="language-ruby"># Original monolith controller
class UsersController &lt; ApplicationController
  def create
    @user = User.new(user_params)
    if @user.save
      # Send welcome email
      UserMailer.welcome_email(@user).deliver_now
      # Update analytics
      Analytics.track_user_signup(@user)
      # Create default preferences
      UserPreferences.create_defaults(@user)
      render json: @user, status: :created
    else
      render json: @user.errors, status: :unprocessable_entity
    end
  end
end
</code></pre>
<p>In the microservices version, this became:</p>
<pre><code class="language-ruby"># User service
class UsersController &lt; ApplicationController
  def create
    @user = User.new(user_params)
    if @user.save
      # Publish event for other services
      EventPublisher.publish('user.created', @user.attributes)
      render json: @user, status: :created
    else
      render json: @user.errors, status: :unprocessable_entity
    end
  end
end

# Email service (separate application)
class UserEventHandler
  def self.handle_user_created(user_data)
    UserMailer.welcome_email(user_data).deliver_now
  end
end

# Analytics service (separate application)
class AnalyticsEventHandler
  def self.handle_user_created(user_data)
    Analytics.track_user_signup(user_data)
  end
end
</code></pre>
<h3>The Reality Check</h3>
<p>The proof of concept revealed some harsh realities:<br />
- Network latency between services was noticeable<br />
- Error handling became much more complex<br />
- Testing required coordinating multiple services<br />
- Monitoring and debugging were significantly more challenging</p>
<p>I realized we needed much more than just splitting up our codebase.</p>
<h2>Phase 2: Infrastructure and Tooling</h2>
<h3>Containerization Journey</h3>
<p>We started with Docker, but I quickly learned that containerizing a monolith is different from containerizing microservices:</p>
<pre><code class="language-dockerfile"># Our first attempt - not optimal
FROM ruby:2.7
WORKDIR /app
COPY Gemfile Gemfile.lock ./
RUN bundle install
COPY . .
EXPOSE 3000
CMD [&quot;rails&quot;, &quot;server&quot;, &quot;-b&quot;, &quot;0.0.0.0&quot;]
</code></pre>
<p>After several iterations and learning about multi-stage builds, image optimization, and security best practices:</p>
<pre><code class="language-dockerfile"># Optimized version
FROM ruby:2.7-alpine AS builder
WORKDIR /app
COPY Gemfile Gemfile.lock ./
RUN apk add --no-cache build-base postgresql-dev \
    &amp;&amp; bundle install --without development test \
    &amp;&amp; rm -rf /usr/local/bundle/cache

FROM ruby:2.7-alpine
RUN apk add --no-cache postgresql-client
WORKDIR /app
COPY --from=builder /usr/local/bundle /usr/local/bundle
COPY . .
RUN adduser -D -s /bin/sh appuser
USER appuser
EXPOSE 3000
CMD [&quot;rails&quot;, &quot;server&quot;, &quot;-b&quot;, &quot;0.0.0.0&quot;]
</code></pre>
<h3>Service Discovery and Communication</h3>
<p>We implemented a service registry using Consul:</p>
<pre><code class="language-ruby"># Service registration
class ServiceRegistry
  def self.register(service_name, host, port)
    consul = Consul.new(url: ENV['CONSUL_URL'])
    consul.agent.service.register(
      name: service_name,
      address: host,
      port: port,
      check: {
        http: &quot;http://#{host}:#{port}/health&quot;,
        interval: &quot;10s&quot;
      }
    )
  end

  def self.discover(service_name)
    consul = Consul.new(url: ENV['CONSUL_URL'])
    services = consul.health.service(service_name, passing: true)
    services.map { |s| &quot;#{s.service.address}:#{s.service.port}&quot; }
  end
end

# HTTP client with service discovery
class ServiceClient
  def initialize(service_name)
    @service_name = service_name
    @client = HTTPClient.new
  end

  def get(path)
    endpoint = ServiceRegistry.discover(@service_name).sample
    @client.get(&quot;http://#{endpoint}#{path}&quot;)
  rescue =&gt; e
    Logger.error(&quot;Failed to call #{@service_name}: #{e.message}&quot;)
    raise ServiceUnavailableError, &quot;#{@service_name} is not available&quot;
  end
end
</code></pre>
<h3>API Gateway Implementation</h3>
<p>We built a simple API gateway using Nginx and Lua:</p>
<pre><code class="language-nginx"># nginx.conf
upstream user_service {
    server user-service:3000;
}

upstream order_service {
    server order-service:3000;
}

server {
    listen 80;

    location /api/v1/users {
        proxy_pass http://user_service;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    location /api/v1/orders {
        proxy_pass http://order_service;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
</code></pre>
<h2>Phase 3: Data Management Challenges</h2>
<h3>The Database Dilemma</h3>
<p>One of the biggest challenges was deciding how to handle our shared database. We had three options:<br />
1. Keep a shared database (anti-pattern but pragmatic)<br />
2. Split the database along service boundaries<br />
3. Use a hybrid approach</p>
<p>We chose the hybrid approach, gradually splitting the database:</p>
<pre><code class="language-ruby"># Migration strategy
class DatabaseMigrationService
  def self.migrate_user_data
    # 1. Dual write to both old and new databases
    # 2. Backfill historical data
    # 3. Switch reads to new database
    # 4. Stop writing to old database

    ActiveRecord::Base.transaction do
      # Write to old database
      user = User.create!(user_params)

      # Write to new database
      begin
        UserService.create_user(user.attributes)
      rescue =&gt; e
        # Handle failure - maybe retry or log for later processing
        Logger.error(&quot;Failed to create user in new system: #{e.message}&quot;)
      end

      user
    end
  end
end
</code></pre>
<h3>Event Sourcing Experiment</h3>
<p>I spent weeks learning about event sourcing and CQRS, implementing a small example:</p>
<pre><code class="language-ruby"># Event store
class EventStore
  def append_event(stream_id, event)
    Event.create!(
      stream_id: stream_id,
      event_type: event.class.name,
      event_data: event.to_json,
      version: next_version(stream_id)
    )
  end

  def get_events(stream_id, from_version = 0)
    Event.where(stream_id: stream_id)
         .where('version &gt; ?', from_version)
         .order(:version)
  end
end

# Aggregate root
class Order
  include AggregateRoot

  def initialize(order_id)
    @order_id = order_id
    @items = []
    @status = 'pending'
  end

  def add_item(product_id, quantity)
    raise &quot;Cannot add items to #{@status} order&quot; unless @status == 'pending'

    apply_event(ItemAddedEvent.new(
      order_id: @order_id,
      product_id: product_id,
      quantity: quantity
    ))
  end

  def apply_item_added_event(event)
    @items &lt;&lt; { product_id: event.product_id, quantity: event.quantity }
  end
end
</code></pre>
<p>While fascinating, event sourcing proved too complex for our immediate needs, but the learning experience was invaluable.</p>
<h2>Phase 4: Observability and Monitoring</h2>
<h3>Distributed Tracing</h3>
<p>Implementing distributed tracing was eye-opening. Suddenly, we could see how requests flowed through our system:</p>
<pre><code class="language-ruby"># Tracing middleware
class TracingMiddleware
  def initialize(app)
    @app = app
  end

  def call(env)
    trace_id = env['HTTP_X_TRACE_ID'] || SecureRandom.uuid
    span_id = SecureRandom.uuid

    # Start span
    span = tracer.start_span(
      operation_name: &quot;#{env['REQUEST_METHOD']} #{env['PATH_INFO']}&quot;,
      child_of: extract_span_context(env),
      tags: {
        'component' =&gt; 'web-server',
        'http.method' =&gt; env['REQUEST_METHOD'],
        'http.url' =&gt; env['PATH_INFO']
      }
    )

    # Add to request context
    env['trace_id'] = trace_id
    env['span_id'] = span_id

    status, headers, body = @app.call(env)

    # Finish span
    span.set_tag('http.status_code', status)
    span.finish

    [status, headers, body]
  end
end
</code></pre>
<h3>Metrics and Alerting</h3>
<p>We instrumented our services with Prometheus metrics:</p>
<pre><code class="language-ruby"># Metrics collector
class MetricsCollector
  def self.setup
    @http_requests = Prometheus::Client::Counter.new(
      :http_requests_total,
      docstring: 'Total HTTP requests',
      labels: [:method, :path, :status]
    )

    @http_duration = Prometheus::Client::Histogram.new(
      :http_request_duration_seconds,
      docstring: 'HTTP request duration',
      labels: [:method, :path],
      buckets: [0.1, 0.5, 1, 2, 5, 10]
    )

    Prometheus::Client.registry.register(@http_requests)
    Prometheus::Client.registry.register(@http_duration)
  end

  def self.record_request(method, path, status, duration)
    @http_requests.increment(labels: { method: method, path: path, status: status })
    @http_duration.observe(duration, labels: { method: method, path: path })
  end
end
</code></pre>
<h2>Phase 5: Cultural and Team Changes</h2>
<h3>Conway's Law in Action</h3>
<p>I learned about Conway's Law the hard way: "Organizations design systems that mirror their own communication structure." Our initial service boundaries didn't work because they didn't match our team structure.</p>
<p>We had to reorganize:<br />
- User Management Team: 2 developers<br />
- Order Processing Team: 3 developers<br />
- Inventory Team: 2 developers<br />
- Payment Team: 2 developers<br />
- Platform Team: 3 developers (me included)</p>
<h3>New Development Practices</h3>
<p>We implemented new practices:<br />
- Service contracts and API versioning<br />
- Consumer-driven contract testing<br />
- Chaos engineering experiments<br />
- Blameless post-mortems</p>
<pre><code class="language-ruby"># Contract testing example
RSpec.describe 'User Service Contract' do
  it 'provides user data in expected format' do
    user_data = UserService.get_user('123')

    expect(user_data).to match({
      id: String,
      email: String,
      created_at: String,
      updated_at: String
    })
  end
end
</code></pre>
<h2>The Painful Lessons</h2>
<h3>Distributed Systems Are Hard</h3>
<p>Our first major outage taught us about cascading failures. A memory leak in the user service caused it to become slow, which caused timeouts in the order service, which caused request queues to back up, bringing down the entire system.</p>
<h3>Network Partitions Are Real</h3>
<p>We experienced our first network partition during a database failover. Half our services couldn't reach the database, but the other half could. The system behaved in unexpected ways, and we learned the importance of partition tolerance.</p>
<h3>Debugging Becomes Complex</h3>
<p>What used to be a simple stack trace became a distributed debugging challenge. We had to invest heavily in logging, tracing, and monitoring tools.</p>
<h2>The Breakthrough Moments</h2>
<h3>Everything Clicked</h3>
<p>After months of struggle, there was a moment when everything clicked. We deployed a new feature that required changes to three services. Each team worked independently, deployed their changes separately, and everything worked seamlessly. The parallel development capability we'd dreamed of was finally real.</p>
<h3>Performance Improvements</h3>
<p>We were able to scale individual services based on demand. Our user service needed more CPU, our inventory service needed more memory, and our analytics service needed more storage. We could optimize each service independently.</p>
<h3>Technology Diversity</h3>
<p>Different teams started experimenting with different technologies. The analytics team moved to Python for better data science libraries. The payment team adopted Go for better performance. The user management team stuck with Ruby but upgraded to the latest version.</p>
<h2>Measuring Success</h2>
<p>After one year of migration:<br />
- Deployment frequency: From once a week to multiple times per day<br />
- Lead time: From weeks to days<br />
- Mean time to recovery: From hours to minutes<br />
- Change failure rate: From 20% to 5%</p>
<p>The spatiotemporal modeling principles we applied helped us understand the timing and dependencies between services, while implementing lightweight engines for specific service functions improved overall system performance. The multi-modal data integration across services ensured consistent data flow and reduced coupling.</p>
<h2>Key Takeaways</h2>
<h3>Technical Lessons</h3>
<ol>
<li><strong>Start with the monolith</strong>: Don't build microservices from day one</li>
<li><strong>Invest in tooling</strong>: Good monitoring, logging, and deployment tools are essential</li>
<li><strong>API design matters</strong>: Spend time designing stable, versioned APIs</li>
<li><strong>Data consistency is hard</strong>: Understand the trade-offs between consistency and availability</li>
</ol>
<h3>Organizational Lessons</h3>
<ol>
<li><strong>Conway's Law is real</strong>: Align your architecture with your team structure</li>
<li><strong>Cultural change takes time</strong>: Technical changes are easier than cultural ones</li>
<li><strong>Embrace failure</strong>: Use failures as learning opportunities</li>
<li><strong>Documentation is crucial</strong>: In a distributed system, clear documentation is vital</li>
</ol>
<h3>Personal Growth</h3>
<ol>
<li><strong>Systems thinking</strong>: I learned to think about systems, not just code</li>
<li><strong>Cross-functional collaboration</strong>: Working with ops, security, and other teams became routine</li>
<li><strong>Patience with complexity</strong>: I learned to embrace and manage complexity rather than fight it</li>
<li><strong>Continuous learning</strong>: The technology landscape changes rapidly; continuous learning is essential</li>
</ol>
<h2>The Journey Continues</h2>
<p>We're now two years into our microservices journey, and I'm still learning every day. We've added service mesh, implemented GitOps, and are exploring serverless architectures. The journey from monolith to microservices taught me that great architecture isn't about following patterns—it's about understanding trade-offs and making informed decisions.</p>
<p>The most important lesson? There's no perfect architecture. There are only architectures that are more or less suitable for your specific context, constraints, and goals. The key is to keep learning, keep adapting, and never stop asking, "Is this still the right approach for us?"</p>
<p>This experience fundamentally changed how I approach software development. I now think in terms of systems, not just applications. I consider operational concerns from the beginning, not as an afterthought. And I've learned to appreciate the complexity of distributed systems while working to manage that complexity effectively.</p>
<p>The journey from monolith to microservices is more than a technical migration—it's a transformation in how we think about building software systems. And that transformation, with all its challenges and rewards, has been one of the most valuable learning experiences of my career.</p>
                </div>
            </article>
        </div>
    </main>
    
    <footer>
        <p>&copy; 2025 我的博客. All rights reserved.</p>
    </footer>
</body>
</html>